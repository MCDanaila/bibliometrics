# Detailed Implementation Plans for Pipeline Improvements

## Introduction

This document provides detailed, actionable task lists for implementing the improvements proposed in `IMPROVEMENTS.md`. Each section corresponds to a suggested enhancement, broken down into manageable steps to guide the development process.

---

## 1. Data Ingestion Enhancements

### 1.1. Memory-Efficient XML Parsing

**Goal:** Reduce memory footprint during XML parsing to handle larger files and improve overall system stability.

1.  **Analyze Current Parsers:**
    *   [ ] Identify all Python scripts involved in XML parsing (e.g., `libbiblio/sources/scopus/scopus_parser.py`, `libbiblio/sources/wos/wos_parser.py`).
    *   [ ] For each parser, document the current XML parsing library and method being used (e.g., `xml.etree.ElementTree.parse()`, `lxml.etree.parse()`).
    *   [ ] Profile memory usage of a sample ingestion run with current parsers to establish a baseline. Tools: `memory-profiler` Python library.
2.  **Research & Select Iterative Parsing Method:**
    *   [ ] Evaluate `xml.etree.ElementTree.iterparse()` and `lxml.etree.iterparse()`. Prioritize `lxml` if not already in use, due to its generally better performance and feature set.
    *   [ ] Confirm compatibility with the structure of Scopus and WoS XML data.
3.  **Refactor Parsers:**
    *   [ ] For each identified parser:
        *   [ ] Create a new branch in version control for changes.
        *   [ ] Modify the code to use the selected iterative parsing method (e.g., `iterparse`). This involves handling events (start/end of elements) and clearing elements from memory once processed (`elem.clear()`).
        *   [ ] Ensure all necessary data extraction logic is maintained.
4.  **Unit Testing:**
    *   [ ] Develop or update unit tests for the refactored parsers using sample XML snippets.
    *   [ ] Verify that data is extracted correctly and completely.
5.  **Integration & Performance Testing:**
    *   [ ] Integrate refactored parsers into a test environment.
    *   [ ] Run ingestion tests with a representative dataset (including large XML files).
    *   [ ] Profile memory usage and compare against the baseline. Aim for a significant reduction.
    *   [ ] Validate data integrity in the staging tables.
6.  **Code Review & Merge:**
    *   [ ] Conduct a code review of the changes.
    *   [ ] Merge the approved changes into the main development branch.
7.  **Documentation:**
    *   [ ] Briefly document the new parsing approach in `TECHNICAL_DOCUMENTATION.md`.

### 1.2. Direct Streaming from ZIP Archives

**Goal:** Reduce disk I/O and temporary storage by parsing XML files directly from ZIP archives.

1.  **Analyze Current ZIP Handling:**
    *   [ ] Identify how ZIP files are currently processed (e.g., full extraction to a temporary directory by `scopus_ingest_zip` or `wos_ingest_zip`).
    *   [ ] Note the libraries used (e.g., `zipfile` Python module).
2.  **Develop Streaming Logic:**
    *   [ ] For each provider's ZIP ingestor:
        *   [ ] Modify the code to use `zipfile.ZipFile` to open the archive.
        *   [ ] Iterate through `ZipInfo` objects within the archive.
        *   [ ] For each XML file, use `ZipFile.open()` to get a file-like object.
        *   [ ] Pass this file-like object directly to the (now memory-efficient) XML parser from step 1.1.
3.  **Error Handling:**
    *   [ ] Implement robust error handling for:
        *   [ ] Corrupt ZIP archives.
        *   [ ] Password-protected ZIPs (if applicable, though unlikely for these sources).
        *   [ ] Non-XML files within ZIPs.
4.  **Testing:**
    *   [ ] Create test cases with various ZIP structures:
        *   [ ] Single XML per ZIP.
        *   [ ] Multiple XMLs per ZIP.
        *   [ ] ZIPs containing non-XML files.
        *   [ ] Empty ZIPs.
        *   [ ] (Optional) Corrupt ZIP for error handling test.
    *   [ ] Verify that all XML files are processed correctly and data integrity is maintained.
    *   [ ] Test impact on overall ingestion speed and disk I/O (e.g., using `iostat`).
5.  **Code Review & Merge:**
    *   [ ] Conduct a code review.
    *   [ ] Merge changes.
6.  **Documentation:**
    *   [ ] Update relevant sections in `TECHNICAL_DOCUMENTATION.md` and `README.md` if usage changes.

### 1.3. Optimized BCP File Handling

**Goal:** Improve efficiency of data loading into PostgreSQL by optimizing BCP file generation and management.

1.  **Analyze Current BCP Generation:**
    *   [ ] Determine how BCP files are currently generated by `ingest_data.py` and provider-specific ingestors.
    *   [ ] Note the typical size and number of BCP files generated per input ZIP/XML.
    *   [ ] Identify the location of `work_dir/bcp` and assess if it's on fast local storage or NAS.
2.  **Strategy Development - BCP Aggregation:**
    *   [ ] Decide on an aggregation strategy:
        *   Option A: One set of BCP files per input ZIP archive (aggregating data from all XMLs within that ZIP).
        *   Option B: Time-based or size-based BCP file rotation (e.g., a new BCP file is created every N minutes or when it reaches X MB).
    *   [ ] Consider the impact on the `load_daemon.sh` script.
3.  **Modify BCP Generation Logic:**
    *   [ ] Update Python scripts to implement the chosen aggregation strategy. This may involve accumulating data in memory (be mindful of overall memory usage) or using temporary intermediate files before finalizing a BCP.
    *   [ ] Ensure BCP files are correctly formatted for PostgreSQL `COPY`.
4.  **Optimize `work_dir/bcp` Location:**
    *   [ ] If `work_dir/bcp` is currently on NAS, evaluate moving it to fast local SSD storage on the processing machine. Update `process.sh` if necessary.
5.  **Performance Testing:**
    *   [ ] Establish a baseline for BCP loading times with the current setup.
    *   [ ] Test the new BCP handling strategy with varying data volumes.
    *   [ ] Monitor `COPY` command performance in PostgreSQL logs.
    *   [ ] Compare overall ingestion pipeline throughput.
6.  **Error Handling:**
    *   [ ] Ensure robust error handling for BCP file creation (e.g., disk full, permission issues).
7.  **Code Review & Merge:**
    *   [ ] Review changes.
    *   [ ] Merge.
8.  **Documentation:**
    *   [ ] Document the new BCP handling strategy.

### 1.4. Enhanced Parallelism Control & Monitoring

**Goal:** Optimize resource utilization and provide better insights into the ingestion process by dynamically adjusting parallelism and improving monitoring.

1.  **Research Monitoring Tools/Libraries:**
    *   [ ] Investigate Python libraries for system load monitoring (e.g., `psutil`) to check CPU, memory, I/O.
    *   [ ] Consider how `process.sh` can access this information or if a separate Python monitoring daemon is needed.
2.  **Design Dynamic Worker Adjustment Logic:**
    *   [ ] Define thresholds for system load (e.g., CPU > 80%, memory available < 500MB, high I/O wait times).
    *   [ ] Design logic for `process.sh` (or a new monitor script) to:
        *   [ ] Periodically check system load.
        *   [ ] Reduce the number of active `ingest_data.py` instances if load is too high.
        *   [ ] Increase instances if load is low and there are pending files in `BibliometricsFileStatus`.
    *   [ ] Ensure a maximum number of workers (`nproc` or configurable) is still respected.
3.  **Implement Changes:**
    *   [ ] Modify `process.sh` to incorporate dynamic adjustment or create a new monitoring script that `process.sh` interacts with.
    *   [ ] Python scripts (`ingest_data.py`) might need to signal their status more actively or be more responsive to termination signals for scale-down.
4.  **Implement Detailed Logging for Workers:**
    *   [ ] In `ingest_data.py`:
        *   [ ] Log start and end of processing for each file.
        *   [ ] Log number of records extracted.
        *   [ ] Log time taken per file.
        *   [ ] Log any significant errors encountered.
    *   [ ] Ensure logs are structured (see section 3.4) and include worker ID if possible.
5.  **Testing:**
    *   [ ] Test the dynamic worker adjustment under various load scenarios (e.g., simulate high CPU load).
    *   [ ] Verify that the number of workers adjusts as expected.
    *   [ ] Review enhanced logs for clarity and completeness.
6.  **Code Review & Merge:**
    *   [ ] Review the complex changes carefully.
    *   [ ] Merge.
7.  **Documentation:**
    *   [ ] Document the new parallelism control mechanism and logging improvements.

---

## 2. Database Interaction & Performance Tuning

### 2.1. Advanced Indexing Strategies

**Goal:** Improve query performance and database manageability through optimized indexing and table partitioning.

1.  **Analyze Current Indexing and Query Performance:**
    *   [ ] Identify the largest tables (e.g., `Publications`, `ScopusAuthorship`, `WoSCitation`) and their current indexes.
    *   [ ] Collect and analyze slow queries from PostgreSQL logs (`log_min_duration_statement`) or by using `EXPLAIN ANALYZE` on queries within `ProcessAllStaging.sh` and common analytical queries.
    *   [ ] Determine if existing indexes are being used effectively.
2.  **Plan Table Partitioning (for very large tables like `Publications`):**
    *   [ ] Choose a partitioning key. For bibliographic data, `publication_year` is often a good candidate.
    *   [ ] Decide on partitioning strategy (e.g., range partitioning by year).
    *   [ ] Plan the process for migrating data from the existing large table to a new partitioned table structure. This might involve:
        *   [ ] Creating parent partitioned table and child partition tables.
        *   [ ] Setting up triggers or rules to route new data.
        *   [ ] Migrating existing data batch by batch.
        *   [ ] This is a significant operation and needs careful planning to minimize downtime or performance impact. Consider tools like `pg_partman` for ongoing partition management.
3.  **Identify Opportunities for Partial Indexes:**
    *   [ ] Look for queries with `WHERE` clauses that frequently filter on a small subset of a table (e.g., `status = 'processed'`, `is_active = TRUE`).
    *   [ ] Design partial indexes for these scenarios.
4.  **Evaluate BRIN Indexes:**
    *   [ ] For very large tables, identify columns that have a strong correlation with their physical storage order (e.g., timestamp columns, monotonically increasing IDs).
    *   [ ] Consider BRIN indexes for these columns as they are much smaller than B-tree indexes.
5.  **Implement Index Changes:**
    *   [ ] Develop scripts to create new indexes and (if chosen) implement partitioning.
    *   [ ] Use `CREATE INDEX CONCURRENTLY` where possible to minimize locking during creation of new indexes on existing tables.
6.  **Testing:**
    *   [ ] Test query performance with new indexes and partitioning using `EXPLAIN ANALYZE`. Compare against baselines.
    *   [ ] Verify that data is correctly routed in partitioned tables.
    *   [ ] Test impact on data loading times and `ProcessAllStaging.sh` execution.
7.  **Code Review & Merge:**
    *   [ ] Review DDL changes and migration scripts carefully.
    *   [ ] Merge.
8.  **Documentation:**
    *   [ ] Document the new indexing strategy and partitioning setup in `TECHNICAL_DOCUMENTATION.md`.
    *   [ ] Update database schema diagrams if any.

### 2.2. Optimize `ProcessAllStaging.sh` SQL Queries

**Goal:** Significantly reduce the execution time of staging scripts by optimizing their SQL queries.

1.  **Identify Bottleneck Queries:**
    *   [ ] For each provider (`scopus`, `wos`), enable detailed query logging or use `EXPLAIN ANALYZE` for every major SQL command within the respective `ProcessAllStaging.sh` script.
    *   [ ] Identify the top 5-10 slowest queries or query steps.
2.  **Analyze Query Plans:**
    *   [ ] For each bottleneck query, carefully examine its execution plan (`EXPLAIN ANALYZE` output).
    *   [ ] Look for:
        *   [ ] Full table scans on large tables.
        *   [ ] Inefficient join methods (e.g., nested loops where hash joins would be better).
        *   [ ] Poor cardinality estimates.
        *   [ ] Unnecessary operations or subqueries.
3.  **Iterative Refactoring and Optimization:**
    *   [ ] For each identified query:
        *   [ ] **Rewrite SQL:** Try alternative phrasings (e.g., different join orders, using CTEs effectively, replacing subqueries with joins).
        *   [ ] **Break Down Complexity:** If a query is very complex, break it into smaller steps using temporary tables. This can help the planner and make debugging easier.
        *   [ ] **Ensure Up-to-Date Statistics:** Run `ANALYZE table_name` on relevant tables before testing query changes, especially after significant data loads or schema changes.
        *   [ ] **Adjust `work_mem`:** For sessions running these large batch SQLs, experiment with increasing `work_mem` in PostgreSQL (if memory allows) to enable more efficient in-memory sorts and hash joins. This can be set per session.
4.  **Testing:**
    *   [ ] Test each refactored query for correctness (ensure it produces the same results as the original).
    *   [ ] Measure performance improvement against the baseline.
    *   [ ] Test the entire `ProcessAllStaging.sh` script after optimizations.
5.  **Code Review & Merge:**
    *   [ ] Review SQL changes carefully, focusing on both logic and performance.
    *   [ ] Merge.
6.  **Documentation:**
    *   [ ] Document significant query optimizations made, perhaps with before/after query plans or performance numbers if the improvement is substantial.

### 2.3. Incremental Staging and Processing

**Goal:** For "update" runs, drastically reduce processing time by only processing newly staged data and its direct dependencies.

1.  **Design Delta Identification Logic:**
    *   [ ] Determine how to identify new records in staging tables that require processing (e.g., based on ingestion timestamps, sequence IDs, or by comparing against production tables).
    *   [ ] For relationships (e.g., linking publications to authors), identify how to find existing related records in production tables.
2.  **Modify Staging Scripts (`ProcessAllStaging.sh`):**
    *   [ ] This is a complex task. For each step in the staging scripts:
        *   [ ] Adapt queries to operate primarily on the "delta" of new records.
        *   [ ] For updates to existing production records (e.g., adding a new author to an existing publication), ensure these are handled correctly.
        *   [ ] Consider using flags or timestamps in staging tables to mark records that have been processed into production.
3.  **Dependency Management:**
    *   [ ] Carefully map out dependencies. For example, if a new publication is added, its authors, affiliations, and citations also need to be processed or linked.
4.  **Testing:**
    *   [ ] Develop comprehensive test cases for various update scenarios:
        *   [ ] New publications with new authors.
        *   [ ] New publications with existing authors.
        *   [ ] Updates to existing publications (e.g., new citations).
    *   [ ] Verify data consistency and completeness in production tables after an incremental run.
    *   [ ] Compare performance against a full "refresh" run on the same new data.
5.  **Code Review & Merge:**
    *   [ ] This will require very careful review due to its complexity.
    *   [ ] Merge.
6.  **Documentation:**
    *   [ ] Thoroughly document the incremental processing logic.

### 2.4. Database Connection Pooling

**Goal:** Reduce overhead of establishing database connections if Python scripts make frequent, short-lived connections. (Note: Current `ingest_data.py` design seems to run longer tasks, so this might be lower priority unless other scripts are making many connections).

1.  **Analyze Connection Patterns:**
    *   [ ] Review `ingest_data.py` and any other Python scripts interacting with the database.
    *   [ ] Determine if they make many short-lived connections or fewer long-lived ones. The `file_status_handler.py` might be a candidate if `get_next_file` and `set_as_processed` open/close connections each time.
2.  **Select Pooling Mechanism (if needed):**
    *   [ ] If pooling is beneficial:
        *   [ ] For `psycopg2`, evaluate `psycopg2.pool.SimpleConnectionPool` or `ThreadedConnectionPool`.
        *   [ ] Consider an external pooler like PgBouncer for application-wide pooling if multiple applications or many short-lived scripts access the database.
3.  **Implement Pooling:**
    *   [ ] Modify Python scripts to acquire and release connections from the pool.
    *   [ ] Ensure proper handling of connection errors from the pool.
4.  **Configuration:**
    *   [ ] Configure pool parameters (min/max connections, timeouts).
5.  **Testing:**
    *   [ ] Test database interactions with pooling enabled.
    *   [ ] Monitor database connection counts.
    *   [ ] (Optional) Stress test to see performance impact, though benefits are mainly for high connection rates.
6.  **Code Review & Merge:**
    *   [ ] Review pooling implementation.
    *   [ ] Merge.
7.  **Documentation:**
    *   [ ] Document the use of connection pooling if implemented.

### 2.5. Concurrent Index Creation

**Goal:** Minimize locking during index creation, allowing other database operations to proceed.

1.  **Identify Suitable Indexes:**
    *   [ ] Review the `CreateAllIndexes.sh` scripts for each provider.
    *   [ ] Identify all `CREATE INDEX` statements.
2.  **Modify Scripts:**
    *   [ ] Change `CREATE INDEX index_name ON table_name ...` to `CREATE INDEX CONCURRENTLY index_name ON table_name ...`.
3.  **Understand Implications:**
    *   [ ] `CREATE INDEX CONCURRENTLY` takes longer and consumes more resources.
    *   [ ] It has different failure modes; a failure might leave an invalid index that needs to be manually dropped.
    *   [ ] It cannot be run inside a transaction block (the scripts might need adjustment if they wrap DDL in transactions).
4.  **Plan Execution:**
    *   [ ] Schedule concurrent index creation during periods when some table write activity is expected but full locking is undesirable.
    *   [ ] Ensure monitoring is in place to check for successful completion or failures.
5.  **Testing:**
    *   [ ] Test `CREATE INDEX CONCURRENTLY` in a development/staging environment.
    *   [ ] Verify that indexes are created correctly.
    *   [ ] Simulate some write load on the table while the index is being created to confirm non-blocking behavior.
6.  **Code Review & Merge:**
    *   [ ] Review script changes.
    *   [ ] Merge.
7.  **Documentation:**
    *   [ ] Note the use of `CONCURRENTLY` in `TECHNICAL_DOCUMENTATION.md`.

---

## 3. Error Handling, Resilience & Logging

### 3.1. Granular Error Tracking & Dead Letter Queues

**Goal:** Prevent entire large files/batches from failing due to minor data errors; isolate problematic data for review.

1.  **Design Error Data Storage:**
    *   [ ] Define a schema for one or more "dead letter" tables. These tables will store information about data that failed parsing or validation.
    *   [ ] Include columns for: original file name, record identifier (if any), timestamp of error, error message, snippet of problematic data, provider.
2.  **Modify Ingestion Parsers (`scopus_parser.py`, `wos_parser.py`, etc.):**
    *   [ ] Implement `try-except` blocks around individual record parsing logic (e.g., parsing a single publication entry within an XML).
    *   [ ] If an error occurs:
        *   [ ] Log the error with details (see Structured Logging 3.4).
        *   [ ] Write the problematic data snippet and error details to the dead letter table.
        *   [ ] Allow the parser to continue processing the next record in the file.
3.  **Update `BibliometricsFileStatus`:**
    *   [ ] Consider adding a new status like 'processed_with_errors' to `BibliometricsFileStatus` if a file completes but had some record-level errors.
    *   [ ] Alternatively, add columns to `BibliometricsFileStatus` to store counts of successful vs. failed records for a file.
4.  **Develop Review Process for Dead Letter Data:**
    *   [ ] Create scripts or procedures for data stewards to periodically review data in the dead letter tables.
    *   [ ] Define a process for correcting and re-processing data if possible, or for acknowledging and archiving known issues.
5.  **Testing:**
    *   [ ] Create sample data files with known errors.
    *   [ ] Verify that valid data is processed and loaded correctly.
    *   [ ] Verify that erroneous data is captured in the dead letter tables with correct details.
    *   [ ] Test that `BibliometricsFileStatus` reflects processing status accurately.
6.  **Code Review & Merge:**
    *   [ ] Review error handling logic and dead letter table schema.
    *   [ ] Merge.
7.  **Documentation:**
    *   [ ] Document the dead letter queue mechanism and review process.

### 3.2. Robust Retry Mechanisms

**Goal:** Improve pipeline resilience by automatically retrying operations that fail due to transient issues.

1.  **Identify Key Operations for Retries:**
    *   [ ] File access from NAS in `ingest_data.py` (e.g., opening ZIP, reading XML).
    *   [ ] Database connections in `file_status_handler.py` or `ingest_data.py`.
    *   [ ] Specific SQL commands in `load_daemon.sh` or `ProcessAllStaging.sh` if they are prone to deadlock or temporary failures (less common for batch).
2.  **Select Retry Strategy:**
    *   [ ] Choose an appropriate strategy, e.g., exponential backoff with jitter. This prevents overwhelming the target system with immediate, rapid retries.
    *   [ ] Define configurable parameters: max number of retries, initial backoff interval, max backoff interval.
3.  **Implement Retry Logic:**
    *   **Python:** Use libraries like `tenacity` or implement a custom retry decorator/wrapper function in `ingest_data.py` and `file_status_handler.py`.
    *   **Shell Scripts:** For scripts like `load_daemon.sh` or `process.sh` (if they call commands that might need retries, e.g., `psql`), implement retry loops in bash.
4.  **Logging for Retries:**
    *   [ ] Log each retry attempt, including the error that triggered it and the delay before the next attempt.
    *   [ ] Log when an operation succeeds after retries, or when it ultimately fails after all retries are exhausted.
5.  **Testing:**
    *   [ ] Develop methods to simulate transient errors:
        *   [ ] Temporarily make a file unavailable on NAS.
        *   [ ] Briefly stop/restart the PostgreSQL server or block connections.
    *   [ ] Verify that retry logic engages as expected and that operations eventually succeed when the transient issue is resolved.
    *   [ ] Test that operations fail correctly after exhausting all retries.
6.  **Code Review & Merge:**
    *   [ ] Review retry logic and configuration.
    *   [ ] Merge.
7.  **Documentation:**
    *   [ ] Document which operations have retries and how they are configured.

### 3.3. Checkpointing & Resumability for `ProcessAllStaging.sh`

**Goal:** Allow long-running staging scripts to resume from the last completed step after a failure, saving significant processing time.

1.  **Analyze Staging Scripts (`ProcessAllStaging.sh`):**
    *   [ ] For each provider, break down the `ProcessAllStaging.sh` script into a sequence of logical, idempotent steps. An idempotent step produces the same result whether run once or multiple times.
    *   [ ] Example steps: "Load Authors from Staging", "Load Publications from Staging", "Link Authors to Publications", "Generate Citation Links".
2.  **Design Progress Tracking Mechanism:**
    *   [ ] Create a new database table (e.g., `StagingProcessSteps`) to track the status of each step.
    *   [ ] Columns: `provider`, `script_name` (e.g., `ProcessAllStaging.sh`), `step_name`, `status` ('pending', 'running', 'completed', 'failed'), `last_updated_timestamp`.
3.  **Modify Staging Scripts:**
    *   [ ] At the beginning of `ProcessAllStaging.sh`:
        *   [ ] Query the `StagingProcessSteps` table to find the last successfully completed step for the current provider.
    *   [ ] For each logical step in the script:
        *   [ ] Before execution: Update its status to 'running' in `StagingProcessSteps`.
        *   [ ] If the step completes successfully: Update its status to 'completed'.
        *   [ ] If the step fails: Update its status to 'failed' and exit the script (or log and halt).
        *   [ ] The script should be modified to skip steps already marked as 'completed'.
4.  **Error Handling for Steps:**
    *   [ ] Ensure each step has robust error handling. If a step fails, it should not leave the database in an inconsistent state that prevents a clean restart of that step.
5.  **Testing:**
    *   [ ] Test normal execution: all steps complete and are marked as such.
    *   [ ] Test failure scenarios:
        *   [ ] Simulate a failure midway through `ProcessAllStaging.sh` (e.g., by manually killing the script or introducing an error in a SQL command).
        *   [ ] Verify that the correct step is marked as 'failed' or 'running'.
        *   [ ] Re-run `ProcessAllStaging.sh`. Verify that it skips completed steps and resumes from the failed/pending step.
    *   [ ] Test idempotency of each step.
6.  **Code Review & Merge:**
    *   [ ] This is a complex change; review carefully.
    *   [ ] Merge.
7.  **Documentation:**
    *   [ ] Document the checkpointing mechanism and how to monitor/reset staging progress if needed.

### 3.4. Structured Logging

**Goal:** Improve diagnosability and monitoring by implementing consistent, machine-parseable logging.

1.  **Choose Logging Format:**
    *   [ ] Select a structured logging format, typically JSON.
2.  **Configure Python Logging:**
    *   [ ] Use Python's `logging` module.
    *   [ ] Configure a `JSONFormatter` (e.g., using `python-json-logger`).
    *   [ ] Ensure all log messages include:
        *   [ ] Timestamp (ISO 8601).
        *   [ ] Log level (INFO, ERROR, DEBUG).
        *   [ ] Logger name (e.g., `ingest_data.py`, `scopus_parser`).
        *   [ ] Message.
        *   [ ] Contextual information: `provider`, `input_file_name`, `worker_id` (if applicable), function name, line number.
3.  **Update Shell Script Logging:**
    *   [ ] For important log messages in shell scripts (`process.sh`, `load_daemon.sh`, etc.):
        *   [ ] Use the `logger` utility if available and appropriate for sending messages to syslog (which can be configured to output JSON).
        *   [ ] Alternatively, write custom shell functions that output log messages in the chosen JSON format to stdout/stderr.
        *   [ ] Include similar contextual information as Python logs where possible.
4.  **Log Aggregation (Optional but Recommended):**
    *   [ ] Plan for log aggregation using tools like:
        *   [ ] ELK Stack (Elasticsearch, Logstash, Kibana).
        *   [ ] Grafana Loki.
        *   [ ] Cloud-based logging services.
    *   [ ] Configure systems to forward logs to the chosen aggregator.
5.  **Testing:**
    *   [ ] Run the pipeline and verify that logs are generated in the correct structured format.
    *   [ ] Check that all desired contextual information is present.
    *   [ ] If using a log aggregator, verify that logs are being collected and are searchable/analyzable.
6.  **Code Review & Merge:**
    *   [ ] Review logging configurations and log message content.
    *   [ ] Merge.
7.  **Documentation:**
    *   [ ] Document the logging format, key fields, and how to access/analyze logs.

---

## 4. Scalability & Architecture (Future Growth Considerations)

### 4.1. Distributed Task Queues

**Goal:** Enable horizontal scaling of ingestion workers across multiple machines for future growth. (This is a Proof-of-Concept outline).

1.  **Research & Select Technology Stack:**
    *   [ ] **Task Queue:** Evaluate Celery (Python-based, mature) or other alternatives like Dramatiq or RQ.
    *   [ ] **Message Broker:** Choose a broker compatible with the selected task queue (e.g., RabbitMQ or Redis for Celery).
    *   [ ] **Result Backend (Optional):** If task results need to be stored (e.g., Redis, database).
2.  **Proof-of-Concept (PoC) Design:**
    *   [ ] Identify a small, self-contained part of the ingestion process to be converted into a distributed task (e.g., processing a single XML file, or even a single ZIP file).
    *   [ ] Define the task function signature and expected inputs/outputs.
3.  **Setup PoC Environment:**
    *   [ ] Install the chosen task queue library, message broker, and result backend on development/test machines.
    *   [ ] Configure Celery workers (or equivalent for other systems).
4.  **Implement PoC Task:**
    *   [ ] Refactor the selected part of `ingest_data.py` or a provider-specific ingestor (e.g., `scopus_ingest_xml`) into a Celery task.
    *   [ ] The task should take necessary parameters (e.g., file path, BCP directory).
5.  **Develop Task Dispatcher:**
    *   [ ] Create a script (or modify `process.sh` / `ingest_data.py`) that discovers input files (as it does now from `BibliometricsFileStatus`) and dispatches them as tasks to the message broker.
6.  **Testing PoC:**
    *   [ ] Run Celery workers on one or more machines.
    *   [ ] Run the task dispatcher script.
    *   [ ] Verify that tasks are picked up by workers and executed.
    *   [ ] Check that BCP files are generated correctly.
    *   [ ] Monitor the task queue and workers.
7.  **Evaluate PoC:**
    *   [ ] Assess complexity of implementation and management.
    *   [ ] Measure performance difference (though PoC might not be fully optimized).
    *   [ ] Identify challenges and areas for further development if this path is pursued.
8.  **Documentation:**
    *   [ ] Document the PoC setup, findings, and recommendations.

### 4.2. Consider Distributed Processing Frameworks

**Goal:** Evaluate the feasibility of using frameworks like Spark or Dask for very large-scale data processing or complex transformations if current methods hit limits. (This is an Evaluation outline).

1.  **Identify Candidate Workloads:**
    *   [ ] Pinpoint parts of the pipeline that are extremely data-intensive or involve complex transformations that might be difficult or inefficient in SQL or single-node Python.
    *   [ ] Examples: Large-scale graph analysis of citation networks, complex deduplication logic across entire datasets, advanced NLP on text fields.
2.  **Research & Select Framework:**
    *   [ ] **Apache Spark:** Mature, feature-rich, good for very large datasets and complex ETL. Supports Scala, Java, Python, R.
    *   [ ] **Dask:** Native Python, integrates well with existing Python libraries (Pandas, NumPy, Scikit-learn), good for parallelizing Python code.
    *   [ ] Consider factors like existing team skills, ecosystem, and specific workload needs.
3.  **Proof-of-Concept (PoC) Design:**
    *   [ ] Choose one identified candidate workload for the PoC.
    *   [ ] Define how the data would be read (e.g., from files, from database tables after initial BCP load).
    *   [ ] Outline the transformation logic in the chosen framework.
4.  **Setup PoC Environment:**
    *   [ ] Set up a small Spark or Dask cluster (can be local mode on a single machine for initial PoC).
    *   [ ] Prepare sample data.
5.  **Implement PoC:**
    *   [ ] Write the PoC code using Spark or Dask APIs.
    *   [ ] Focus on demonstrating the core processing logic.
6.  **Testing PoC:**
    *   [ ] Run the PoC on sample data.
    *   [ ] Verify correctness of results.
    *   [ ] Get a preliminary idea of performance characteristics and resource usage.
7.  **Evaluate PoC:**
    *   [ ] Compare complexity vs. current approach.
    *   [ ] Estimate potential performance gains for the full dataset.
    *   [ ] Assess development effort, learning curve, and operational overhead.
    *   [ ] Determine if the benefits justify a significant architectural shift for the identified workload(s).
8.  **Documentation:**
    *   [ ] Document the PoC setup, findings, benchmark results (if any), and recommendations.

---

## 5. Configuration & Maintainability

### 5.1. Centralized Configuration File

**Goal:** Improve clarity and ease of management for pipeline parameters.

1.  **Define Configuration Parameters:**
    *   [ ] List all parameters currently set via environment variables in `db/set_biblio_db_envs.sh` (e.g., DB host, user, pass, db_name).
    *   [ ] Identify other hardcoded or script-level parameters that could be externalized (e.g., paths in `process.sh`, `bcp_dir`, parallel worker counts, retry settings from section 3.2, logging levels).
2.  **Choose Configuration File Format:**
    *   [ ] Select a suitable format: YAML (human-readable, good for nested structures), TOML (simple, clear), INI (traditional). JSON is also an option but less human-friendly for comments.
    *   [ ] For this project, YAML or TOML are good candidates.
3.  **Design Configuration File Structure:**
    *   [ ] Organize parameters into logical sections (e.g., `database`, `paths`, `ingestion_settings`, `logging`).
    *   [ ] Include comments in the template/example configuration file to explain each parameter.
4.  **Implement Configuration Loading:**
    *   **Python:** Use libraries like `PyYAML` (for YAML), `toml` (for TOML), or `configparser` (for INI) to load and parse the configuration file in `ingest_data.py`, `file_status_handler.py`, etc.
    *   **Shell Scripts:**
        *   [ ] For shell scripts (`process.sh`, etc.), use a parser tool (e.g., `yq` for YAML, or specific bash parsers for INI/TOML) or write a small Python/utility script that reads the config and exports values as environment variables for the shell script to consume.
        *   [ ] Alternatively, `process.sh` could source a script that translates the config file into environment variables.
5.  **Refactor Scripts to Use Configuration:**
    *   [ ] Replace direct environment variable usage (e.g., `$POSTGRES_DB`) or hardcoded values with values read from the configuration system.
    *   [ ] Remove parameter settings from `db/set_biblio_db_envs.sh` once they are moved to the config file (it might still be used for sourcing functions, or totally replaced).
6.  **Create Default/Template Configuration File:**
    *   [ ] Provide a template (e.g., `config.template.yaml`) with all parameters and placeholder/default values.
    *   [ ] Instruct users to copy and customize this template (e.g., to `config.yaml`).
    *   [ ] Ensure `config.yaml` (or equivalent) is added to `.gitignore`.
7.  **Testing:**
    *   [ ] Test the pipeline with all parameters being read from the new configuration file.
    *   [ ] Verify that default values are handled correctly.
    *   [ ] Test with overridden values.
8.  **Documentation:**
    *   [ ] Document the new configuration file format, all parameters, and how to set them up in `README.md` and `TECHNICAL_DOCUMENTATION.md`.
    *   [ ] Update setup instructions.

### 5.2. Code Refactoring and Testing

**Goal:** Improve code quality, readability, maintainability, and reliability through refactoring and comprehensive testing.

1.  **Shell Script Refactoring (`process.sh`, `load_daemon.sh`, etc.):**
    *   [ ] Identify overly long or complex sections in shell scripts.
    *   [ ] Break these down into smaller, well-defined functions within the same script or move them to separate utility shell scripts that can be sourced.
    *   [ ] Add more comments and clear variable naming.
    *   [ ] Ensure consistent error handling and exit codes.
2.  **Python Code Refactoring (General):**
    *   [ ] Apply standard Python best practices (PEP 8).
    *   [ ] Identify complex functions or classes in `ingest_data.py`, parsers, ingestors, and `file_status_handler.py`. Refactor for clarity and single responsibility.
    *   [ ] Improve variable names and add comments where logic is non-obvious.
3.  **Develop Unit Tests (Python):**
    *   [ ] **Parsers:** Write unit tests for XML/data parsing logic in provider-specific parsers. Use small, focused sample XML/data snippets to test specific extraction rules and edge cases.
    *   [ ] **Transformations:** If there's significant data transformation logic in Python, unit test these functions.
    *   [ ] **Utility Functions:** Unit test helper functions in `file_status_handler.py` or other utility modules.
    *   [ ] Use a testing framework like `pytest` or `unittest`.
    *   [ ] Aim for good test coverage of critical Python code.
4.  **Develop Integration Tests:**
    *   [ ] Design integration tests for key parts of the pipeline:
        *   [ ] Test ingestion of a single ZIP/XML file through to BCP generation.
        *   [ ] Test BCP loading into staging tables (`load_daemon.sh` functionality).
        *   [ ] Test a small-scale run of `ProcessAllStaging.sh` with sample staging data.
    *   [ ] These tests will involve multiple components and require a test database environment.
    *   [ ] Automate these tests if possible.
5.  **Static Code Analysis:**
    *   [ ] Integrate static analysis tools into the development workflow:
        *   [ ] `flake8` or `pylint` for Python.
        *   [ ] `shellcheck` for shell scripts.
    *   [ ] Address reported issues.
6.  **Continuous Integration (CI) Setup (Optional but Recommended):**
    *   [ ] Set up a CI pipeline (e.g., using GitHub Actions, GitLab CI) to automatically:
        *   [ ] Run static analysis.
        *   [ ] Run unit tests.
        *   [ ] Run integration tests (if feasible in CI environment).
7.  **Code Review Standards:**
    *   [ ] Enforce code reviews for all changes, focusing on clarity, correctness, performance, and test coverage.
8.  **Documentation:**
    *   [ ] Document testing strategies and how to run tests.
    *   [ ] Update code comments and existing documentation as code is refactored.

---

## Conclusion

Successfully implementing these task lists will lead to a more robust, performant, and scalable bibliographic data processing pipeline. Prioritization should align with the most critical bottlenecks identified in the current system.
