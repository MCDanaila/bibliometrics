{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Connected to DB: <bound method PSQL_DB.__repr__ of Postgres('biblioowner', <password hidden>, 'id-hdb-psgr-cp46.ethz.ch', '5432', 'bibliometrics')>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from utils import  parse_contributors, parse_leitzahl, get_missing_leitzahls\n",
    "\n",
    "from constants import CKONSORG_COLUMNS, LEITZAHL_COLUMNS_EXT, PUBLICATION_COLUMNS_EXT, CKONSORG_TABLE, INPUT_DATA_PATH, OUT_BCP_PATH, ALL_COLUMNS_SORTED, PUBLICATION_COLUMNS, PUBLICATION_TABLE, AUTHORSHIP_COLUMNS, AUTHORSHIP_TABLE, MASTER_TABLE, LEITZAHL_COLUMNS, LEITZHAL_TABLE\n",
    "from db.db_importer import dump_table, change_column_type_to_int\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download\n",
    "\n",
    "Download the data per year from Research Collection API with a developer APIKey (see `download_pipeline.ipynb`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data parsing \n",
    "\n",
    "1. Read and load the data in pandas Dataframes\n",
    "2. Extract and parse different columns as separated entities from the dataframe\n",
    "3. Write these entities in bcp files\n",
    "4. Run bash scripts to import bcp files in postgres database (**REQUIRED**: *sql table creation scripts already executed*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load data in pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "INPUT_DATA_PATH = '/home/bibliometric/data/research_collection/input/'\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def bulk_load_df() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load multiple CSV files from the input directory into a single DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A concatenated DataFrame containing data from all CSV files.\n",
    "    \"\"\"\n",
    "    df_list: List[pd.DataFrame] = []\n",
    "\n",
    "    for file_name in sorted(Path(INPUT_DATA_PATH).iterdir()):\n",
    "        if file_name.is_file() and file_name.suffix == '.csv':\n",
    "            try:\n",
    "                logging.info(f'Reading file: {file_name}')\n",
    "                df_list.append(pd.read_csv(file_name, index_col=False, low_memory=False, dtype=str))\n",
    "            except pd.errors.EmptyDataError:\n",
    "                logging.error(f'EmptyDataError - {file_name} is empty and will be skipped.')\n",
    "            except pd.errors.ParserError:\n",
    "                logging.error(f'ParserError - {file_name} is malformed and will be skipped.')\n",
    "            except Exception as error:\n",
    "                logging.error(f'Error reading {file_name} => {error}')\n",
    "    \n",
    "    if df_list:\n",
    "        logging.info('Concatenating dataframes')\n",
    "        return pd.concat(df_list, ignore_index=True)\n",
    "    else:\n",
    "        logging.warning('No dataframes to concatenate, returning an empty DataFrame.')\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2-4 Extract, parse, migrate to DB\n",
    "\n",
    "Extract the relative columns used in the SQL tables from the dataframe and parse them if needed. \n",
    "\n",
    "Write the dataframe in bcp files ready to be imported in Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.x.1 Extract publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_publication(df: pd.DataFrame, reload: bool = True, sep:str = '\\t'):\n",
    "\tBCP_FILE = 'publications.bcp'\n",
    "\tbcp_file_path = OUT_BCP_PATH+BCP_FILE\n",
    "\tif reload:\n",
    "\t\tpublications_df = df[df.columns.intersection(set(PUBLICATION_COLUMNS))]\n",
    "\t\tpublications_df = publications_df.reindex(columns=PUBLICATION_COLUMNS)\n",
    "\t\tpublications_df['rc_year'] = publications_df['dc_date_issued'].str.extract(r'([0-9][0-9][0-9][0-9])', expand=True)\n",
    "\t\tpublications_df.to_csv(bcp_file_path, sep=sep, index=False, index_label='\\t', header=False)\n",
    "\tprint(f'== INFO - DONE - Wrote publications in file {bcp_file_path=}')\n",
    "\t#dump_table(table_name=PUBLICATION_TABLE, bcp_file=bcp_file_path, columns=PUBLICATION_COLUMNS)\n",
    "\ttryAgain = -1\n",
    "\tcount = 0\n",
    "\tskipped_file_path = OUT_BCP_PATH+'skipped_pub.bcp'\n",
    "\twhile tryAgain != 0:\n",
    "\t\tprint(f\"== START Try Again {count=}\")\n",
    "\t\ttryAgain = int(dump_table(table_name=PUBLICATION_TABLE, bcp_file=bcp_file_path, columns=PUBLICATION_COLUMNS, sep=sep))\n",
    "\t\tprint(f'{tryAgain=}')\n",
    "\t\tif tryAgain > 0:\n",
    "\t\t\tcount += 1\n",
    "\t\t\tskipped_row = ''\n",
    "\t\t\twith open( bcp_file_path, \"r+\" ) as f:\n",
    "\t\t\t\tlines = f.readlines()\n",
    "\t\t\t\tf.seek(0)\n",
    "\t\t\t\tskipped_row = lines.pop( tryAgain - 1 )\n",
    "\t\t\t\tprint(f\"INFO - Removed {skipped_row=}\")\n",
    "\t\t\t\tf.truncate()\n",
    "\t\t\t\tf.writelines( lines ) \n",
    "\t\t\twith open(skipped_file_path, \"a\" ) as f2:\n",
    "\t\t\t\tf2.write(skipped_row)\n",
    "\telse:\n",
    "\t\tprint(\"== Try Again DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.x.2 Extract contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_autorship(df: pd.DataFrame):\n",
    "\tBCP_FILE = 'authorship.bcp'\n",
    "\tbcp_file_path = OUT_BCP_PATH+BCP_FILE\n",
    "\t#global_authors_set = set()\n",
    "\tcontributors_set, local_authorship_list = parse_contributors(df)\n",
    "\t#global_authors_set.update(contributors_set)\n",
    "\t#pd.DataFrame(global_authors_set).to_csv('./data/output/author.bcp', sep='\\t', index=False, index_label='\\t', header=False)\n",
    "\tglobal_authorship_df = pd.DataFrame(local_authorship_list, columns=AUTHORSHIP_COLUMNS)\n",
    "\tglobal_authorship_df.to_csv(bcp_file_path, sep='\\t', index=False, index_label='\\t', header=False)\n",
    "\tprint(f'== INFO - DONE - Wrote authorships in file {bcp_file_path=}')\n",
    "\t#dump_table(table_name=AUTHORSHIP_TABLE,  bcp_file=bcp_file_path, columns=AUTHORSHIP_COLUMNS)\n",
    "\ttryAgain = -1\n",
    "\tcount = 0\n",
    "\tskipped_file_path = OUT_BCP_PATH+'skipped_aut.bcp'\n",
    "\twhile tryAgain != 0:\n",
    "\t\tprint(f\"== START Try Again {count=}\")\n",
    "\t\ttryAgain = int(dump_table(table_name=AUTHORSHIP_TABLE,  bcp_file=bcp_file_path, columns=AUTHORSHIP_COLUMNS))\n",
    "\t\tprint(f'{tryAgain=}')\n",
    "\t\tif tryAgain > 0:\n",
    "\t\t\tcount += 1\n",
    "\t\t\tskipped_row = ''\n",
    "\t\t\twith open( bcp_file_path, \"r+\" ) as f:\n",
    "\t\t\t\tlines = f.readlines()\n",
    "\t\t\t\tf.seek(0)\n",
    "\t\t\t\tskipped_row = lines.pop( tryAgain - 1 )\n",
    "\t\t\t\tprint(f\"INFO - Removed {skipped_row=}\")\n",
    "\t\t\t\tf.truncate()\n",
    "\t\t\t\tf.writelines( lines ) \n",
    "\t\t\twith open(skipped_file_path, \"a\" ) as f2:\n",
    "\t\t\t\tf2.write(skipped_row)\n",
    "\telse:\n",
    "\t\tprint(\"== Try Again DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_master(df: pd.DataFrame, reload: bool = True, drop_table: bool = False, sep: str = '\\t'):\n",
    "    BCP_FILE = 'master.bcp'\n",
    "    bcp_file_path = OUT_BCP_PATH + BCP_FILE\n",
    "    skipped_file_path = OUT_BCP_PATH + 'skipped.bcp'\n",
    "\n",
    "    if reload:\n",
    "        df.to_csv(bcp_file_path, sep=sep, index=False, header=False)\n",
    "        print(f'== INFO - DONE - Wrote master table to file {bcp_file_path}')\n",
    "\n",
    "    try_again = -1\n",
    "    count = 0\n",
    "\n",
    "    while try_again != 0:\n",
    "        print(f\"== START Try Again {count}\")\n",
    "        try_again = dump_table(table_name=MASTER_TABLE, bcp_file=bcp_file_path, columns=df.columns.to_list(), drop_table=drop_table, sep=sep)\n",
    "        if try_again > 0:\n",
    "            count += 1\n",
    "            with open(bcp_file_path, \"r+\") as f:\n",
    "                lines = f.readlines()\n",
    "                f.seek(0)\n",
    "                skipped_row = lines.pop(try_again - 1)\n",
    "                print(f\"INFO - Removed row: {skipped_row.strip()}\")\n",
    "                f.truncate()\n",
    "                f.writelines(lines)\n",
    "            with open(skipped_file_path, \"a\") as f2:\n",
    "                f2.write(skipped_row)\n",
    "        elif try_again == -1:\n",
    "            print(\"ERROR - Unrecoverable error during dumping table\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"== Try Again DONE\")\n",
    "\n",
    "\"\"\" def parse_master(df: pd.DataFrame, reload: bool = True, drop_table: bool = False, sep:str = '\\t'):\n",
    "\tBCP_FILE = 'master.bcp'\n",
    "\tbcp_file_path = OUT_BCP_PATH+BCP_FILE\n",
    "\tskipped_file_path = OUT_BCP_PATH+'skipped.bcp'\n",
    "    \n",
    "    if reload:\n",
    "\t\t#df.to_csv('master.csv')\n",
    "\t\tdf.to_csv(bcp_file_path, sep=sep, index=False, index_label='\\t', header=False)\n",
    "\t\tprint(f'== INFO - DONE - Wrote master table in file {bcp_file_path=}')\n",
    "\t#dump_table(table_name=MASTER_TABLE, bcp_file=bcp_file_path, columns=df.columns.to_list())\n",
    "\ttryAgain = -1\n",
    "\tcount = 0\n",
    "\t\n",
    "\twhile tryAgain != 0:\n",
    "\t\tprint(f\"== START Try Again {count=}\")\n",
    "\t\ttryAgain = int(dump_table(table_name=MASTER_TABLE, bcp_file=bcp_file_path, columns=df.columns.to_list(), drop_table=drop_table, sep=sep))\n",
    "\t\tprint(f'{tryAgain=}')\n",
    "\t\tif tryAgain > 0:\n",
    "\t\t\tcount += 1\n",
    "\t\t\tskipped_row = ''\n",
    "\t\t\twith open( bcp_file_path, \"r+\" ) as f:\n",
    "\t\t\t\tlines = f.readlines()\n",
    "\t\t\t\tf.seek(0)\n",
    "\t\t\t\tskipped_row = lines.pop( tryAgain - 1 )\n",
    "\t\t\t\tprint(f\"INFO - Removed {skipped_row=}\")\n",
    "\t\t\t\tf.truncate()\n",
    "\t\t\t\tf.writelines( lines ) \n",
    "\t\t\twith open(skipped_file_path, \"a\" ) as f2:\n",
    "\t\t\t\tf2.write(skipped_row)\n",
    "\telse:\n",
    "\t\tprint(\"== Try Again DONE\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from constants import PATH_TO_CKONSORG_DATA, CKONSORG_COLUMNS\n",
    "from utils import get_latest_ckonsorg_filename\n",
    "\n",
    "def parse_ckonsorg_data(dump_table:bool = False):\n",
    "\n",
    "\tckonsorg_file_name = get_latest_ckonsorg_filename()\n",
    "\n",
    "\tdf = pd.read_csv(PATH_TO_CKONSORG_DATA + ckonsorg_file_name, index_col=False, low_memory=False, dtype=str)\n",
    "\t\n",
    "\tdf.columns = [x.lower() for x in df.columns]\n",
    "\n",
    "\t#take the subset needed\n",
    "\tckonsorg_df = df[CKONSORG_COLUMNS]\n",
    "\n",
    "\t# Identify columns that start with 'LZ'\n",
    "\tlz_columns = [col for col in ckonsorg_df.columns if col.startswith('lz')]\n",
    "\n",
    "\t# Replace 'T' with '0' in the filtered columns\n",
    "\tckonsorg_df.loc[:, lz_columns] = ckonsorg_df.loc[:, lz_columns].applymap(lambda x: x.replace('T', '0'))\n",
    "\tckonsorg_df['vondat'] = ckonsorg_df['vondat'].str.extract(r'([0-9]{4})', expand=True)\n",
    "\tckonsorg_df['bisdat'] = ckonsorg_df['bisdat'].str.extract(r'([0-9]{4})', expand=True)\n",
    "\tckonsorg_df.head(10)\n",
    "\n",
    "\tckonsorg_bcp_file = 'ckonsorg.bcp'\n",
    "\tbcp_file_path = PATH_TO_CKONSORG_DATA + ckonsorg_bcp_file\n",
    "\tckonsorg_df.to_csv(bcp_file_path, sep='\\t', index=False, header=False)\n",
    "\tprint('Created BCP file: ', bcp_file_path)\n",
    "\tif dump_table:\n",
    "\t\tdump_table(table_name=CKONSORG_TABLE, bcp_file=bcp_file_path, columns=CKONSORG_COLUMNS, drop_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_skipped(skipped_file: str = 'skipped_pub.bcp', \n",
    "\t\t\t\t  error_file: str = 'error.bcp', \n",
    "\t\t\t\t  bcp_file:str = 'publications.bcp', \n",
    "\t\t\t\t  drop_table: bool = True, \n",
    "\t\t\t\t  columns: list = ALL_COLUMNS_SORTED, \n",
    "\t\t\t\t  table_name: str = MASTER_TABLE,\n",
    "\t\t\t\t  sep: str = '\\t'):\n",
    "\tSKIPPED_PATH = OUT_BCP_PATH + skipped_file\n",
    "\tERROR_PATH = OUT_BCP_PATH + error_file\n",
    "\tBCP_PATH = OUT_BCP_PATH + bcp_file\n",
    "\ttryAgain = -1\n",
    "\tcount = 0\n",
    "\twhile tryAgain != 0:\n",
    "\t\tprint(f\"== START Try Again {count=}\")\n",
    "\t\ttryAgain = int(dump_table(table_name=table_name, bcp_file=SKIPPED_PATH, columns=columns, drop_table=drop_table, sep=sep))\n",
    "\t\t#print(f'{tryAgain=}')\n",
    "\t\tif tryAgain > 0:\n",
    "\t\t\tcount += 1\n",
    "\t\t\tskipped_row = ''\n",
    "\t\t\twith open( SKIPPED_PATH, \"r+\" ) as f:\n",
    "\t\t\t\tlines = f.readlines()\n",
    "\t\t\t\tf.seek(0)\n",
    "\t\t\t\tskipped_row = lines.pop( tryAgain - 1 )\n",
    "\t\t\t\tprint(f\"INFO - Removed {skipped_row=}\")\n",
    "\t\t\t\tf.truncate()\n",
    "\t\t\t\tf.writelines( lines ) \n",
    "\t\t\twith open(ERROR_PATH, \"a\" ) as f2:\n",
    "\t\t\t\tf2.write(skipped_row)\n",
    "\telse:\n",
    "\t\tprint(\"== Try Again DONE\")\n",
    "\t\tshutil.copyfileobj(open(SKIPPED_PATH, 'rb'), open(BCP_PATH, 'ab'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "OUT_BCP_PATH2 = Path(OUT_BCP_PATH)\n",
    "\n",
    "def parse_and_dump(\n",
    "    df: pd.DataFrame, \n",
    "    bcp_file_name: str, \n",
    "    table_name: str, \n",
    "    columns: Optional[List[str]] = None, \n",
    "    reload: bool = True, \n",
    "    drop_table: bool = False, \n",
    "    sep: str = '\\t',\n",
    "    extract_year: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generic method to parse DataFrame and dump to a file with error handling.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame to be processed.\n",
    "        bcp_file_name (str): Name of the BCP file.\n",
    "        table_name (str): Name of the table to dump data.\n",
    "        columns (Optional[List[str]]): List of columns to reindex the DataFrame.\n",
    "        reload (bool): Flag to reload data.\n",
    "        drop_table (bool): Flag to drop the table before dumping.\n",
    "        sep (str): Separator for the CSV file.\n",
    "        extract_year (bool): Flag to extract the year from 'dc_date_issued' column.\n",
    "    \"\"\"\n",
    "    bcp_file_path = OUT_BCP_PATH2 / bcp_file_name\n",
    "    skipped_file_path = OUT_BCP_PATH2 / f'skipped_{bcp_file_name}'\n",
    "\n",
    "    if reload:\n",
    "        if columns:\n",
    "            df = df[df.columns.intersection(set(columns))]\n",
    "            df = df.reindex(columns=columns)\n",
    "        if extract_year:\n",
    "            df['rc_year'] = df['dc_date_issued'].str.extract(r'([0-9]{4})', expand=True)\n",
    "        df.to_csv(bcp_file_path, sep=sep, index=False, index_label='\\t', header=False)\n",
    "        logging.info(f'== INFO - DONE - Wrote data to file {bcp_file_path}')\n",
    "\n",
    "    try_again = -1\n",
    "    count = 0\n",
    "\n",
    "    while try_again != 0:\n",
    "        logging.info(f\"== START Try Again {count}\")\n",
    "        try_again = dump_table(table_name=table_name, bcp_file=bcp_file_path, columns=columns or df.columns.to_list(), drop_table=drop_table, sep=sep)\n",
    "        logging.info(f'{try_again=}')\n",
    "        if try_again > 0:\n",
    "            count += 1\n",
    "            with open(bcp_file_path, \"r+\") as f:\n",
    "                lines = f.readlines()\n",
    "                f.seek(0)\n",
    "                skipped_row = lines.pop(try_again - 1)\n",
    "                logging.info(f\"INFO - Removed row: {skipped_row.strip()}\")\n",
    "                f.truncate()\n",
    "                f.writelines(lines)\n",
    "            with open(skipped_file_path, \"a\") as f2:\n",
    "                f2.write(skipped_row)\n",
    "        elif try_again == -1:\n",
    "            logging.error(\"ERROR - Unrecoverable error during dumping table\")\n",
    "            break\n",
    "    else:\n",
    "        logging.info(\"== Try Again DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE\n",
    "probably in this step a few publications are lost in the `count()` because they not contain the `dc_identifier_uri` and no `handle_id` can be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 17:26:44,541 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1905_metadata.csv\n",
      "2024-08-21 17:26:44,551 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1909_metadata.csv\n",
      "2024-08-21 17:26:44,556 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1910_metadata.csv\n",
      "2024-08-21 17:26:44,560 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1911_metadata.csv\n",
      "2024-08-21 17:26:44,564 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1912_metadata.csv\n",
      "2024-08-21 17:26:44,583 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1913_metadata.csv\n",
      "2024-08-21 17:26:44,594 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1914_metadata.csv\n",
      "2024-08-21 17:26:44,599 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1915_metadata.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 17:26:44,608 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1916_metadata.csv\n",
      "2024-08-21 17:26:44,627 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1917_metadata.csv\n",
      "2024-08-21 17:26:44,639 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1918_metadata.csv\n",
      "2024-08-21 17:26:44,645 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1919_metadata.csv\n",
      "2024-08-21 17:26:44,657 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1920_metadata.csv\n",
      "2024-08-21 17:26:44,662 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1921_metadata.csv\n",
      "2024-08-21 17:26:44,667 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1922_metadata.csv\n",
      "2024-08-21 17:26:44,672 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1923_metadata.csv\n",
      "2024-08-21 17:26:44,677 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1924_metadata.csv\n",
      "2024-08-21 17:26:44,685 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1925_metadata.csv\n",
      "2024-08-21 17:26:44,696 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1926_metadata.csv\n",
      "2024-08-21 17:26:44,709 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1927_metadata.csv\n",
      "2024-08-21 17:26:44,715 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1928_metadata.csv\n",
      "2024-08-21 17:26:44,723 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1929_metadata.csv\n",
      "2024-08-21 17:26:44,728 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1930_metadata.csv\n",
      "2024-08-21 17:26:44,733 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1931_metadata.csv\n",
      "2024-08-21 17:26:44,741 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1932_metadata.csv\n",
      "2024-08-21 17:26:44,760 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1933_metadata.csv\n",
      "2024-08-21 17:26:44,768 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1934_metadata.csv\n",
      "2024-08-21 17:26:44,782 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1935_metadata.csv\n",
      "2024-08-21 17:26:44,788 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1936_metadata.csv\n",
      "2024-08-21 17:26:44,794 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1937_metadata.csv\n",
      "2024-08-21 17:26:44,802 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1938_metadata.csv\n",
      "2024-08-21 17:26:44,808 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1939_metadata.csv\n",
      "2024-08-21 17:26:44,822 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1940_metadata.csv\n",
      "2024-08-21 17:26:44,828 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1941_metadata.csv\n",
      "2024-08-21 17:26:44,833 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1942_metadata.csv\n",
      "2024-08-21 17:26:44,853 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1943_metadata.csv\n",
      "2024-08-21 17:26:44,859 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1944_metadata.csv\n",
      "2024-08-21 17:26:44,866 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1945_metadata.csv\n",
      "2024-08-21 17:26:44,877 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1946_metadata.csv\n",
      "2024-08-21 17:26:44,890 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1947_metadata.csv\n",
      "2024-08-21 17:26:44,903 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1948_metadata.csv\n",
      "2024-08-21 17:26:44,913 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1949_metadata.csv\n",
      "2024-08-21 17:26:44,929 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1950_metadata.csv\n",
      "2024-08-21 17:26:44,942 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1951_metadata.csv\n",
      "2024-08-21 17:26:44,953 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1952_metadata.csv\n",
      "2024-08-21 17:26:44,963 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1953_metadata.csv\n",
      "2024-08-21 17:26:44,983 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1954_metadata.csv\n",
      "2024-08-21 17:26:44,992 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1955_metadata.csv\n",
      "2024-08-21 17:26:45,003 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1956_metadata.csv\n",
      "2024-08-21 17:26:45,016 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1957_metadata.csv\n",
      "2024-08-21 17:26:45,028 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1958_metadata.csv\n",
      "2024-08-21 17:26:45,040 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1959_metadata.csv\n",
      "2024-08-21 17:26:45,052 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1960_metadata.csv\n",
      "2024-08-21 17:26:45,070 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1961_metadata.csv\n",
      "2024-08-21 17:26:45,082 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1962_metadata.csv\n",
      "2024-08-21 17:26:45,093 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1963_metadata.csv\n",
      "2024-08-21 17:26:45,104 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1964_metadata.csv\n",
      "2024-08-21 17:26:45,124 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1965_metadata.csv\n",
      "2024-08-21 17:26:45,134 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1966_metadata.csv\n",
      "2024-08-21 17:26:45,144 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1967_metadata.csv\n",
      "2024-08-21 17:26:45,153 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1968_metadata.csv\n",
      "2024-08-21 17:26:45,176 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1969_metadata.csv\n",
      "2024-08-21 17:26:45,185 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1970_metadata.csv\n",
      "2024-08-21 17:26:45,207 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1971_metadata.csv\n",
      "2024-08-21 17:26:45,220 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1972_metadata.csv\n",
      "2024-08-21 17:26:45,232 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1973_metadata.csv\n",
      "2024-08-21 17:26:45,244 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1974_metadata.csv\n",
      "2024-08-21 17:26:45,273 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1975_metadata.csv\n",
      "2024-08-21 17:26:45,284 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1976_metadata.csv\n",
      "2024-08-21 17:26:45,307 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1977_metadata.csv\n",
      "2024-08-21 17:26:45,337 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1978_metadata.csv\n",
      "2024-08-21 17:26:45,376 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1979_metadata.csv\n",
      "2024-08-21 17:26:45,400 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1980_metadata.csv\n",
      "2024-08-21 17:26:45,412 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1981_metadata.csv\n",
      "2024-08-21 17:26:45,452 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1982_metadata.csv\n",
      "2024-08-21 17:26:45,491 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1983_metadata.csv\n",
      "2024-08-21 17:26:45,521 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1984_metadata.csv\n",
      "2024-08-21 17:26:45,552 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1985_metadata.csv\n",
      "2024-08-21 17:26:45,572 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1986_metadata.csv\n",
      "2024-08-21 17:26:45,592 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1987_metadata.csv\n",
      "2024-08-21 17:26:45,632 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1988_metadata.csv\n",
      "2024-08-21 17:26:45,652 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1989_metadata.csv\n",
      "2024-08-21 17:26:45,693 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1990_metadata.csv\n",
      "2024-08-21 17:26:45,734 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1991_metadata.csv\n",
      "2024-08-21 17:26:45,773 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1992_metadata.csv\n",
      "2024-08-21 17:26:45,798 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1993_metadata.csv\n",
      "2024-08-21 17:26:45,864 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1994_metadata.csv\n",
      "2024-08-21 17:26:45,901 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1995_metadata.csv\n",
      "2024-08-21 17:26:45,941 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1996_metadata.csv\n",
      "2024-08-21 17:26:45,985 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1997_metadata.csv\n",
      "2024-08-21 17:26:46,045 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1998_metadata.csv\n",
      "2024-08-21 17:26:46,116 - INFO - Reading file: /home/bibliometric/data/research_collection/input/1999_metadata.csv\n",
      "2024-08-21 17:26:46,181 - INFO - Reading file: /home/bibliometric/data/research_collection/input/2000_metadata.csv\n",
      "2024-08-21 17:26:46,237 - INFO - Reading file: /home/bibliometric/data/research_collection/input/2001_metadata.csv\n",
      "2024-08-21 17:26:46,334 - INFO - Reading file: /home/bibliometric/data/research_collection/input/2002_metadata.csv\n",
      "2024-08-21 17:26:46,421 - INFO - Reading file: /home/bibliometric/data/research_collection/input/2003_metadata.csv\n",
      "2024-08-21 17:26:46,637 - INFO - Reading file: /home/bibliometric/data/research_collection/input/2004_metadata.csv\n",
      "2024-08-21 17:26:46,789 - INFO - Reading file: /home/bibliometric/data/research_collection/input/2005_metadata.csv\n",
      "2024-08-21 17:26:47,111 - INFO - Reading file: /home/bibliometric/data/research_collection/input/2006_metadata.csv\n",
      "2024-08-21 17:26:47,423 - INFO - Reading file: /home/bibliometric/data/research_collection/input/2007_metadata.csv\n",
      "2024-08-21 17:26:47,649 - INFO - Reading file: /home/bibliometric/data/research_collection/input/2008_metadata.csv\n",
      "2024-08-21 17:26:48,021 - INFO - Reading file: /home/bibliometric/data/research_collection/input/2009_metadata.csv\n",
      "2024-08-21 17:26:48,445 - INFO - Reading file: /home/bibliometric/data/research_collection/input/2010_metadata.csv\n",
      "2024-08-21 17:26:48,489 - INFO - Reading file: /home/bibliometric/data/research_collection/input/2011_metadata.csv\n",
      "2024-08-21 17:26:48,844 - INFO - Reading file: /home/bibliometric/data/research_collection/input/2012_metadata.csv\n",
      "2024-08-21 17:26:49,318 - INFO - Reading file: /home/bibliometric/data/research_collection/input/2013_metadata.csv\n",
      "2024-08-21 17:26:49,733 - INFO - Reading file: /home/bibliometric/data/research_collection/input/2014_metadata.csv\n",
      "2024-08-21 17:26:50,222 - INFO - Reading file: /home/bibliometric/data/research_collection/input/2015_metadata.csv\n",
      "2024-08-21 17:26:50,757 - INFO - Reading file: /home/bibliometric/data/research_collection/input/2016_metadata.csv\n",
      "2024-08-21 17:26:51,198 - INFO - Reading file: /home/bibliometric/data/research_collection/input/2017_metadata.csv\n",
      "2024-08-21 17:26:51,539 - INFO - Reading file: /home/bibliometric/data/research_collection/input/2018_metadata.csv\n",
      "2024-08-21 17:26:51,927 - INFO - Reading file: /home/bibliometric/data/research_collection/input/2019_metadata.csv\n",
      "2024-08-21 17:26:52,334 - INFO - Reading file: /home/bibliometric/data/research_collection/input/2020_metadata.csv\n",
      "2024-08-21 17:26:52,897 - INFO - Reading file: /home/bibliometric/data/research_collection/input/2021_metadata.csv\n",
      "2024-08-21 17:26:53,477 - INFO - Reading file: /home/bibliometric/data/research_collection/input/2022_metadata.csv\n",
      "2024-08-21 17:26:54,165 - INFO - Reading file: /home/bibliometric/data/research_collection/input/2023_metadata.csv\n",
      "2024-08-21 17:26:54,677 - INFO - Reading file: /home/bibliometric/data/research_collection/input/2024_metadata.csv\n",
      "2024-08-21 17:26:54,853 - INFO - Concatenating dataframes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 256122 entries, 0 to 256121\n",
      "Columns: 134 entries, dc_contributor_author to ethz_date_openbisupload\n",
      "dtypes: object(134)\n",
      "memory usage: 261.8+ MB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1573717/1609896663.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  big_df['handle_id'] = big_df['dc_identifier_uri'].apply(lambda uri: uri.split(\"/\")[-1] if pd.notna(uri) else uri)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    256120\n",
      "True          2\n",
      "Name: count, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 256122 entries, 0 to 256121\n",
      "Columns: 135 entries, dc_contributor to rc_item_id\n",
      "dtypes: object(135)\n",
      "memory usage: 263.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# thse steps can be done year by year or as a one big dataset\n",
    "big_df = bulk_load_df()\n",
    "reordered_df = big_df.reindex(columns=ALL_COLUMNS_SORTED)\n",
    "print(big_df.info())\n",
    "big_df['ethz_size'] = big_df['ethz_size'].apply(lambda size: size.replace('\\t', \" \") if pd.notna(size) else size)\n",
    "big_df['handle_id'] = big_df['dc_identifier_uri'].apply(lambda uri: uri.split(\"/\")[-1] if pd.notna(uri) else uri)\n",
    "big_df['dc_title'] = big_df['dc_title'].apply(lambda title: title.replace('\\\\', \"\") if pd.notna(title) else title)\n",
    "\n",
    "reordered_df = big_df.reindex(columns=ALL_COLUMNS_SORTED)\n",
    "print(reordered_df.duplicated(subset=['handle_id']).value_counts())\n",
    "#print(reordered_df.duplicated(subset=['handle_id']))\n",
    "reordered_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: \n",
    "1. apparently looks like there were duplicates but checking those are false duplicates because 2 records are `NaN` and one has an older `handle_id`\n",
    "2. checks for `NaN` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicated_rows = reordered_df[reordered_df.duplicated(subset=['handle_id'], keep=False)]\n",
    "#duplicated_rows\n",
    "#null_rows = reordered_df.loc[reordered_df['dc_identifier_uri'].isnull() ] #| reordered_df['dc_identifier_doi'].isnull()]\n",
    "#null_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_leitzhal_df = reordered_df.dropna(subset=['ethz_leitzahl'])[['handle_id', 'rc_item_id','ethz_leitzahl', 'ethz_leitzahlidentifiers', 'ethz_leitzahlidentifiers_certified']]\n",
    "#parse_leitzahl(full_leitzhal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse_and_dump(reordered_df, 'publications.bcp', PUBLICATION_TABLE, PUBLICATION_COLUMNS, extract_year=True, drop_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse_and_dump(reordered_df, 'authorship.bcp', AUTHORSHIP_TABLE, AUTHORSHIP_COLUMNS, drop_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse_and_dump(reordered_df, 'master.bcp', MASTER_TABLE, drop_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse_autorship(reordered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse_publication(reordered_df, True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change_column_type_to_int(PUBLICATION_TABLE, \"rc_year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct the publications with errors (usually there are tabs in titles or descriptions or other text fields) and run parse_skipped for publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from constants import PUBLICATION_COLUMNS\n",
    "#parse_skipped(skipped_file = 'skipped_pub.bcp', error_file = 'error.bcp', bcp_file = 'publications.bcp', drop_table=False, columns=PUBLICATION_COLUMNS, table_name=PUBLICATION_TABLE, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!! RUN THIS MANUALLY after the creation of publication table\n",
    "\n",
    "change year from VARCHAR to INTEGER\n",
    "\n",
    "```sql\n",
    "ALTER TABLE \"RCPublication\"\n",
    "ALTER COLUMN rc_year TYPE INT USING (cast ( coalesce( nullif( trim(rc_year), '' ), '0' ) as integer ))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running parse_master the first time select reload=True, then after correcting all records with errors with the script in the next section, relaunch parse_master with reload=False to avoid overwriting the corrected bcp file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse_master(df=reordered_df, reload=True, drop_table=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run after \"parse_master(big_df, True, '@')\" in order to replace all the @ in string that cause errors when writing to the DB with sep=@"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse_skipped(skipped_file = 'skipped.bcp', error_file = 'error.bcp', bcp_file = 'master.bcp', drop_table = False, columns= ALL_COLUMNS_SORTED, table_name = MASTER_TABLE, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse_ckonsorg_data(dump_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!! RUN THIS MANUALLY after the creation of ckonsorg table\n",
    "\n",
    "```sql\n",
    "ALTER TABLE \"RCckonsorg\"\n",
    "ALTER COLUMN vondat TYPE INT USING (cast ( coalesce( nullif( trim(vondat), '' ), '0' ) as integer ))\n",
    "```\n",
    "\n",
    "```sql\n",
    "ALTER TABLE \"RCckonsorg\"\n",
    "ALTER COLUMN bisdat TYPE INT USING (cast ( coalesce( nullif( trim(bisdat), '' ), '0' ) as integer ))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ckonsorg_bcp_path = '/home/bibliometric/data/research_collection/ckonsorg/ckonsorg.bcp'\n",
    "#ckonsorg_df = pd.read_csv(ckonsorg_bcp_path, sep='\\t', names=CKONSORG_COLUMNS, index_col=False, low_memory=False, dtype=str)\n",
    "#ckonsorg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leitzahl_bcp_path = '/home/bibliometric/data/research_collection/bcp_out/leitzhal.bcp'\n",
    "#full_stored_leitzhal_df = pd.read_csv(leitzahl_bcp_path, sep='\\t', names=LEITZAHL_COLUMNS, index_col=False, low_memory=False, dtype=str)\n",
    "#full_stored_leitzhal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lz_df_w_missing_lz = get_missing_leitzahls(ckonsorg_df, full_stored_leitzhal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_update_lz_bcp = '/home/bibliometric/data/research_collection/bcp_out/ckonsorg_updated_leitzahl.bcp'\n",
    "#lz_df_w_missing_lz.to_csv(path_to_update_lz_bcp, sep='\\t', index=False, index_label='\\t', header=False)\n",
    "#dump_table(LEITZHAL_TABLE, path_to_update_lz_bcp, LEITZAHL_COLUMNS_EXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------- TESTs -------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>handle_id</th>\n",
       "      <th>rc_item_id</th>\n",
       "      <th>ou_code</th>\n",
       "      <th>ou_name</th>\n",
       "      <th>is_leaf</th>\n",
       "      <th>is_certified</th>\n",
       "      <th>lz_lv_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>147275</td>\n",
       "      <td>172054</td>\n",
       "      <td>00007</td>\n",
       "      <td>T-Departemente</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lz70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>147275</td>\n",
       "      <td>172054</td>\n",
       "      <td>00012</td>\n",
       "      <td>T-Lehre + For.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lz80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>147275</td>\n",
       "      <td>172054</td>\n",
       "      <td>00002</td>\n",
       "      <td>T-ETH Zürich</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lz90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55989</td>\n",
       "      <td>442093</td>\n",
       "      <td>02610</td>\n",
       "      <td>T-Verkehrspl.Transp.</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lz50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55989</td>\n",
       "      <td>442093</td>\n",
       "      <td>02115</td>\n",
       "      <td>T-Bau,Umw.u.Geomatik</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lz60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436313</th>\n",
       "      <td>679485</td>\n",
       "      <td>551405</td>\n",
       "      <td>00012</td>\n",
       "      <td>Lehre und Forschung</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lz80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436314</th>\n",
       "      <td>679485</td>\n",
       "      <td>551405</td>\n",
       "      <td>00007</td>\n",
       "      <td>Departemente</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lz70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436315</th>\n",
       "      <td>679485</td>\n",
       "      <td>551405</td>\n",
       "      <td>02140</td>\n",
       "      <td>Dep. Inf.technologie und Elektrotechnik / Dep....</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lz60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436316</th>\n",
       "      <td>679485</td>\n",
       "      <td>551405</td>\n",
       "      <td>02636</td>\n",
       "      <td>Institut fr Integrierte Systeme / Integrated S...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lz50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436317</th>\n",
       "      <td>679485</td>\n",
       "      <td>551405</td>\n",
       "      <td>03996</td>\n",
       "      <td>Benini, Luca / Benini, Luca</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lz40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1436318 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        handle_id rc_item_id ou_code  \\\n",
       "0          147275     172054   00007   \n",
       "1          147275     172054   00012   \n",
       "2          147275     172054   00002   \n",
       "3           55989     442093   02610   \n",
       "4           55989     442093   02115   \n",
       "...           ...        ...     ...   \n",
       "1436313    679485     551405   00012   \n",
       "1436314    679485     551405   00007   \n",
       "1436315    679485     551405   02140   \n",
       "1436316    679485     551405   02636   \n",
       "1436317    679485     551405   03996   \n",
       "\n",
       "                                                   ou_name is_leaf  \\\n",
       "0                                           T-Departemente   False   \n",
       "1                                           T-Lehre + For.   False   \n",
       "2                                             T-ETH Zürich   False   \n",
       "3                                     T-Verkehrspl.Transp.   False   \n",
       "4                                     T-Bau,Umw.u.Geomatik   False   \n",
       "...                                                    ...     ...   \n",
       "1436313                                Lehre und Forschung   False   \n",
       "1436314                                       Departemente   False   \n",
       "1436315  Dep. Inf.technologie und Elektrotechnik / Dep....   False   \n",
       "1436316  Institut fr Integrierte Systeme / Integrated S...   False   \n",
       "1436317                        Benini, Luca / Benini, Luca    True   \n",
       "\n",
       "        is_certified lz_lv_match  \n",
       "0                NaN        lz70  \n",
       "1                NaN        lz80  \n",
       "2                NaN        lz90  \n",
       "3                NaN        lz50  \n",
       "4                NaN        lz60  \n",
       "...              ...         ...  \n",
       "1436313          NaN        lz80  \n",
       "1436314          NaN        lz70  \n",
       "1436315          NaN        lz60  \n",
       "1436316          NaN        lz50  \n",
       "1436317          NaN        lz40  \n",
       "\n",
       "[1436318 rows x 7 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_lz_df = pd.read_csv(path_to_update_lz_bcp, sep='\\t', names=LEITZAHL_COLUMNS_EXT, index_col=False, low_memory=False, dtype=str)\n",
    "full_lz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>handle_id</th>\n",
       "      <th>rc_item_id</th>\n",
       "      <th>dc_type</th>\n",
       "      <th>ethz_eth</th>\n",
       "      <th>ethz_leitzahl</th>\n",
       "      <th>ethz_leitzahl_certified</th>\n",
       "      <th>dc_title</th>\n",
       "      <th>dc_date_issued</th>\n",
       "      <th>rc_year</th>\n",
       "      <th>dc_identifier_uri</th>\n",
       "      <th>ethz_source</th>\n",
       "      <th>dc_identifier_doi</th>\n",
       "      <th>dc_identifier_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>139872</td>\n",
       "      <td>139839</td>\n",
       "      <td>Doctoral Thesis</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eine neue Bestimmung der Molekldimensionen</td>\n",
       "      <td>1905</td>\n",
       "      <td>1905</td>\n",
       "      <td>http://hdl.handle.net/20.500.11850/139872</td>\n",
       "      <td>ECOL</td>\n",
       "      <td>10.3929/ethz-a-000565688</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132784</td>\n",
       "      <td>132751</td>\n",
       "      <td>Doctoral Thesis</td>\n",
       "      <td>yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zur Kenntnis der Chinonimine und der Chinone</td>\n",
       "      <td>1909</td>\n",
       "      <td>1909</td>\n",
       "      <td>http://hdl.handle.net/20.500.11850/132784</td>\n",
       "      <td>ECOL</td>\n",
       "      <td>10.3929/ethz-a-000090494</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131903</td>\n",
       "      <td>131870</td>\n",
       "      <td>Doctoral Thesis</td>\n",
       "      <td>yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ueber Anilinschwarz</td>\n",
       "      <td>1909</td>\n",
       "      <td>1909</td>\n",
       "      <td>http://hdl.handle.net/20.500.11850/131903</td>\n",
       "      <td>ECOL</td>\n",
       "      <td>10.3929/ethz-a-000088717</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132465</td>\n",
       "      <td>132432</td>\n",
       "      <td>Doctoral Thesis</td>\n",
       "      <td>yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ueber den Abbau von Chlorophyll durch Alkalien</td>\n",
       "      <td>1909</td>\n",
       "      <td>1909</td>\n",
       "      <td>http://hdl.handle.net/20.500.11850/132465</td>\n",
       "      <td>ECOL</td>\n",
       "      <td>10.3929/ethz-a-000089765</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>132925</td>\n",
       "      <td>132892</td>\n",
       "      <td>Doctoral Thesis</td>\n",
       "      <td>yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Beitrge zur Kenntnis pharmazeutisch verwendete...</td>\n",
       "      <td>1909</td>\n",
       "      <td>1909</td>\n",
       "      <td>http://hdl.handle.net/20.500.11850/132925</td>\n",
       "      <td>ECOL</td>\n",
       "      <td>10.3929/ethz-a-000090766</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266033</th>\n",
       "      <td>680545</td>\n",
       "      <td>552299</td>\n",
       "      <td>Conference Paper</td>\n",
       "      <td>yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dynamic O(Arboricity) Coloring in Polylogarith...</td>\n",
       "      <td>2024-06</td>\n",
       "      <td>2024</td>\n",
       "      <td>http://hdl.handle.net/20.500.11850/680545</td>\n",
       "      <td>SCOPUS</td>\n",
       "      <td>10.3929/ethz-b-000680545</td>\n",
       "      <td>10.1145/3618260.3649782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266034</th>\n",
       "      <td>680621</td>\n",
       "      <td>552302</td>\n",
       "      <td>Working Paper</td>\n",
       "      <td>yes</td>\n",
       "      <td>ETH Zrich::00002 - ETH Zrich::00012 - Lehre un...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Beyond FITT - How Density Can Improve the Unde...</td>\n",
       "      <td>2024-05-21</td>\n",
       "      <td>2024</td>\n",
       "      <td>http://hdl.handle.net/20.500.11850/680621</td>\n",
       "      <td>FORM</td>\n",
       "      <td>10.3929/ethz-b-000680621</td>\n",
       "      <td>10.51224/SRXIV.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266035</th>\n",
       "      <td>680667</td>\n",
       "      <td>552304</td>\n",
       "      <td>Other Publication</td>\n",
       "      <td>yes</td>\n",
       "      <td>ETH Zrich::00002 - ETH Zrich::00012 - Lehre un...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A Rearmament Challenge: The Example of Gunpowder</td>\n",
       "      <td>2024-07-05</td>\n",
       "      <td>2024</td>\n",
       "      <td>http://hdl.handle.net/20.500.11850/680667</td>\n",
       "      <td>FORM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266036</th>\n",
       "      <td>680073</td>\n",
       "      <td>551880</td>\n",
       "      <td>Journal Article</td>\n",
       "      <td>yes</td>\n",
       "      <td>ETH Zrich::00002 - ETH Zrich::00012 - Lehre un...</td>\n",
       "      <td>ETH Zrich::00002 - ETH Zrich::00012 - Lehre un...</td>\n",
       "      <td>Electric-field activating on-surface tailored ...</td>\n",
       "      <td>2025-01-19</td>\n",
       "      <td>2025</td>\n",
       "      <td>http://hdl.handle.net/20.500.11850/680073</td>\n",
       "      <td>FORM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1016/j.seppur.2024.128291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266037</th>\n",
       "      <td>669251</td>\n",
       "      <td>552026</td>\n",
       "      <td>Book Chapter</td>\n",
       "      <td>yes</td>\n",
       "      <td>['ETH Zrich::00002 - ETH Zrich::00012 - Lehre ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Inferring the evolutionary history of the Sino...</td>\n",
       "      <td>2025-06</td>\n",
       "      <td>2025</td>\n",
       "      <td>http://hdl.handle.net/20.500.11850/669251</td>\n",
       "      <td>FORM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.1144/SP549-2023-174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>266038 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       handle_id rc_item_id            dc_type ethz_eth  \\\n",
       "0         139872     139839    Doctoral Thesis       no   \n",
       "1         132784     132751    Doctoral Thesis      yes   \n",
       "2         131903     131870    Doctoral Thesis      yes   \n",
       "3         132465     132432    Doctoral Thesis      yes   \n",
       "4         132925     132892    Doctoral Thesis      yes   \n",
       "...          ...        ...                ...      ...   \n",
       "266033    680545     552299   Conference Paper      yes   \n",
       "266034    680621     552302      Working Paper      yes   \n",
       "266035    680667     552304  Other Publication      yes   \n",
       "266036    680073     551880    Journal Article      yes   \n",
       "266037    669251     552026       Book Chapter      yes   \n",
       "\n",
       "                                            ethz_leitzahl  \\\n",
       "0                                                     NaN   \n",
       "1                                                     NaN   \n",
       "2                                                     NaN   \n",
       "3                                                     NaN   \n",
       "4                                                     NaN   \n",
       "...                                                   ...   \n",
       "266033                                                NaN   \n",
       "266034  ETH Zrich::00002 - ETH Zrich::00012 - Lehre un...   \n",
       "266035  ETH Zrich::00002 - ETH Zrich::00012 - Lehre un...   \n",
       "266036  ETH Zrich::00002 - ETH Zrich::00012 - Lehre un...   \n",
       "266037  ['ETH Zrich::00002 - ETH Zrich::00012 - Lehre ...   \n",
       "\n",
       "                                  ethz_leitzahl_certified  \\\n",
       "0                                                     NaN   \n",
       "1                                                     NaN   \n",
       "2                                                     NaN   \n",
       "3                                                     NaN   \n",
       "4                                                     NaN   \n",
       "...                                                   ...   \n",
       "266033                                                NaN   \n",
       "266034                                                NaN   \n",
       "266035                                                NaN   \n",
       "266036  ETH Zrich::00002 - ETH Zrich::00012 - Lehre un...   \n",
       "266037                                                NaN   \n",
       "\n",
       "                                                 dc_title dc_date_issued  \\\n",
       "0              Eine neue Bestimmung der Molekldimensionen           1905   \n",
       "1            Zur Kenntnis der Chinonimine und der Chinone           1909   \n",
       "2                                     Ueber Anilinschwarz           1909   \n",
       "3          Ueber den Abbau von Chlorophyll durch Alkalien           1909   \n",
       "4       Beitrge zur Kenntnis pharmazeutisch verwendete...           1909   \n",
       "...                                                   ...            ...   \n",
       "266033  Dynamic O(Arboricity) Coloring in Polylogarith...        2024-06   \n",
       "266034  Beyond FITT - How Density Can Improve the Unde...     2024-05-21   \n",
       "266035   A Rearmament Challenge: The Example of Gunpowder     2024-07-05   \n",
       "266036  Electric-field activating on-surface tailored ...     2025-01-19   \n",
       "266037  Inferring the evolutionary history of the Sino...        2025-06   \n",
       "\n",
       "       rc_year                          dc_identifier_uri ethz_source  \\\n",
       "0         1905  http://hdl.handle.net/20.500.11850/139872        ECOL   \n",
       "1         1909  http://hdl.handle.net/20.500.11850/132784        ECOL   \n",
       "2         1909  http://hdl.handle.net/20.500.11850/131903        ECOL   \n",
       "3         1909  http://hdl.handle.net/20.500.11850/132465        ECOL   \n",
       "4         1909  http://hdl.handle.net/20.500.11850/132925        ECOL   \n",
       "...        ...                                        ...         ...   \n",
       "266033    2024  http://hdl.handle.net/20.500.11850/680545      SCOPUS   \n",
       "266034    2024  http://hdl.handle.net/20.500.11850/680621        FORM   \n",
       "266035    2024  http://hdl.handle.net/20.500.11850/680667        FORM   \n",
       "266036    2025  http://hdl.handle.net/20.500.11850/680073        FORM   \n",
       "266037    2025  http://hdl.handle.net/20.500.11850/669251        FORM   \n",
       "\n",
       "               dc_identifier_doi           dc_identifier_other  \n",
       "0       10.3929/ethz-a-000565688                           NaN  \n",
       "1       10.3929/ethz-a-000090494                           NaN  \n",
       "2       10.3929/ethz-a-000088717                           NaN  \n",
       "3       10.3929/ethz-a-000089765                           NaN  \n",
       "4       10.3929/ethz-a-000090766                           NaN  \n",
       "...                          ...                           ...  \n",
       "266033  10.3929/ethz-b-000680545       10.1145/3618260.3649782  \n",
       "266034  10.3929/ethz-b-000680621            10.51224/SRXIV.411  \n",
       "266035                       NaN                           NaN  \n",
       "266036                       NaN  10.1016/j.seppur.2024.128291  \n",
       "266037                       NaN        10.1144/SP549-2023-174  \n",
       "\n",
       "[266038 rows x 13 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publications_bcp_path = '/home/bibliometric/data/research_collection/bcp_out/publications.bcp'\n",
    "\n",
    "publications_df = pd.read_csv(publications_bcp_path, sep='\\t', names=PUBLICATION_COLUMNS, index_col=False, low_memory=False, dtype=str)\n",
    "publications_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_ou_codes = set(zip(full_lz_df['handle_id'], full_lz_df['rc_item_id']))\n",
    "\n",
    "# Filter full_lz_df to include only the relevant handle_id and rc_item_id pairs\n",
    "filtered_lz_df = full_lz_df[full_lz_df.set_index(['handle_id', 'rc_item_id']).index.isin(existing_ou_codes)]\n",
    "\n",
    "# Group by handle_id and rc_item_id and aggregate ou_code into lists\n",
    "grouped_lz_df = filtered_lz_df.groupby(['handle_id', 'rc_item_id'])['ou_code'].agg(set).reset_index()\n",
    "\n",
    "# Merge the aggregated lists back to publications_df\n",
    "merged_df = publications_df.merge(grouped_lz_df, on=['handle_id', 'rc_item_id'], how='left')\n",
    "\n",
    "# Add the resulting lists of ou_code to a new column in publications_df\n",
    "publications_df['pub_lz'] = merged_df['ou_code'].where(\n",
    "    merged_df[['handle_id', 'rc_item_id']].apply(tuple, axis=1).isin(existing_ou_codes),\n",
    "    None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== INFO - Executed table creation for RCPublication\n",
      "== INFO - cursor and connection CLOSED.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BCP_FILE = '/home/bibliometric/data/research_collection/bcp_out/publications_extended.bcp'\n",
    "#bcp_file_path = OUT_BCP_PATH+BCP_FILE\n",
    "publications_df.to_csv(BCP_FILE, sep='\\t', index=False, index_label='\\t', header=False)\n",
    "dump_table(PUBLICATION_TABLE, BCP_FILE, PUBLICATION_COLUMNS_EXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stop \u001b[38;5;241m=\u001b[39m \u001b[43mget_error\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_error' is not defined"
     ]
    }
   ],
   "source": [
    "stop = get_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the 'lz' columns in ckonsorg_df\n",
    "lz_columns = [col for col in ckonsorg_df.columns if col.startswith('lz')]\n",
    "\n",
    "# Flatten the ckonsorg_df: Create a DataFrame with 'ou_code', 'lz_match', and 'source_index' columns\n",
    "flat_df = pd.DataFrame()\n",
    "\n",
    "for lz_col in lz_columns:\n",
    "    temp_df = ckonsorg_df[[lz_col]].copy()\n",
    "    temp_df['lz_match'] = lz_col\n",
    "    #temp_df['source_index'] = temp_df.index  # Capture the source index\n",
    "    temp_df = temp_df.rename(columns={lz_col: 'ou_code'})\n",
    "    flat_df = pd.concat([flat_df, temp_df])\n",
    "\n",
    "# Drop duplicates to avoid redundant merges\n",
    "flat_df = flat_df.drop_duplicates(subset=['ou_code'])\n",
    "\n",
    "# Merge leitzahl_df with flat_df on 'ou_code'\n",
    "leitzahl_df = full_stored_leitzhal_df.merge(flat_df, on='ou_code', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck_columns = {'lz90':'na90','lz80':'na80','lz70':'na70','lz60':'na60','lz50':'na50','lz40':'na40','lz30':'na30','lz20':'na20'}\n",
    "ck_keys_list = list(ck_columns).index('lz50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leitzahl_row_to_add = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-based lookup for faster membership checking\n",
    "existing_ou_codes = set(zip(leitzahl_df['handle_id'], leitzahl_df['rc_item_id'], leitzahl_df['ou_code']))\n",
    "\n",
    "filtered_leitzahl_df = leitzahl_df[leitzahl_df.is_leaf == True]\n",
    "\n",
    "for idx, leitzahl_row in filtered_leitzahl_df.iterrows():\n",
    "\t#print(idx, leitzahl_row)\n",
    "\tckonsorg_row = ckonsorg_df[ckonsorg_df[leitzahl_row.lz_match].str.contains(leitzahl_row.ou_code)]\n",
    "\t#print('----------- ckonsorg_row')\n",
    "\t#print(ckonsorg_row)\n",
    "\tlz_match_idx = list(ck_columns).index(leitzahl_row.lz_match)\n",
    "\tfor i in range(1, lz_match_idx+1):\n",
    "\t\tck_parent_key = list(ck_columns)[lz_match_idx-i]\n",
    "\t\tck_parent_value = ckonsorg_row[ck_parent_key].values[0]\n",
    "\t\t#print('-------------------------')\n",
    "\t\t#print(\"ck_parent_key = \", ck_parent_key)\n",
    "\t\t#print('ckonsorg_row[ck_parent_key].values = ', ckonsorg_row[ck_parent_key].values[0])\n",
    "\t\t#if (not ((leitzahl_df['handle_id'] == leitzahl_row.handle_id) & (leitzahl_df['rc_item_id'] == leitzahl_row.rc_item_id) & (leitzahl_df['ou_code'] == ck_parent_value)).any()):\n",
    "\t\tif (leitzahl_row.handle_id, leitzahl_row.rc_item_id, ck_parent_value) not in existing_ou_codes:\n",
    "\t\t\t#print('isPresent: ', ((leitzahl_df['handle_id'] == leitzahl_row.handle_id) & (leitzahl_df['rc_item_id'] == leitzahl_row.rc_item_id) & (leitzahl_df['ou_code'] == ck_parent_value)).any())\n",
    "\t\t\tleitzahl_row_to_add.append({\n",
    "\t\t\t\t'handle_id':leitzahl_row.handle_id, \n",
    "\t\t\t\t'rc_item_id': leitzahl_row.rc_item_id, \n",
    "\t\t\t\t'ou_code': ck_parent_value,\n",
    "\t\t\t\t'ou_name': ckonsorg_row[ck_columns.get(ck_parent_key)].values[0],\n",
    "\t\t\t\t'is_leaf':\tFalse,\n",
    "\t\t\t\t'is_certified': None,\n",
    "\t\t\t\t'lz_match':\tck_parent_key})\n",
    "\t\t\texisting_ou_codes.add((leitzahl_row.handle_id, leitzahl_row.rc_item_id, ck_parent_value))\n",
    "\t\"\"\" if idx > 100:\n",
    "\t\tbreak \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leitzahl_row_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leitzahl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_update_lz_bcp = '/home/bibliometric/mihai_test/bibliometrics/libbiblio/sources/rc/ckonsorg_updated_leitzahl.bcp'\n",
    "df3 = pd.concat([pd.DataFrame(leitzahl_row_to_add), leitzahl_df], ignore_index=True)\n",
    "df3.to_csv(\"./new_leit.bcp\", sep='\\t', index=False, index_label='\\t', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dump_table(LEITZHAL_TABLE, path_to_update_lz_bcp, LEITZAHL_COLUMNS_EXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.value_counts('lz_match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leitzahl_df.value_counts('lz_match')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
