{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Connected to DB: <bound method PSQL_DB.__repr__ of Postgres('biblioowner', <password hidden>, 'id-hdb-psgr-cp46.ethz.ch', '5432', 'bibliometrics')>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "from utils import  parse_contributors, parse_leitzahl\n",
    "\n",
    "from constants import INPUT_DATA_PATH, OUT_BCP_PATH, ALL_COLUMNS_SORTED, PUBLICATION_COLUMNS, PUBLICATION_TABLE, AUTHORSHIP_COLUMNS, AUTHORSHIP_TABLE, MASTER_TABLE\n",
    "from db.db_importer import dump_table\n",
    "\n",
    "# Get current date in yyyy_mm_dd format\n",
    "current_date = datetime.now().strftime('%Y_%m_%d')\n",
    "\n",
    "# Configure logging\n",
    "log_dir = 'logs'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file_path = os.path.join(log_dir, f'{current_date}_dump_processing.log')\n",
    "\n",
    "logger = logging.getLogger('metadata_logger')\n",
    "if not logger.hasHandlers():\n",
    "    logger.setLevel(logging.INFO)\n",
    "    file_handler = logging.FileHandler(log_file_path)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_load_df() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load multiple CSV files from the input directory into a single DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A concatenated DataFrame containing data from all CSV files.\n",
    "    \"\"\"\n",
    "    df_list: List[pd.DataFrame] = []\n",
    "\n",
    "    for file_name in sorted(Path(INPUT_DATA_PATH).iterdir()):\n",
    "        if file_name.is_file() and file_name.suffix == '.csv':\n",
    "            try:\n",
    "                logging.info(f'Reading file: {file_name}')\n",
    "                df_list.append(pd.read_csv(file_name, index_col=False, low_memory=False, dtype=str))\n",
    "            except pd.errors.EmptyDataError:\n",
    "                logging.error(f'EmptyDataError - {file_name} is empty and will be skipped.')\n",
    "            except pd.errors.ParserError:\n",
    "                logging.error(f'ParserError - {file_name} is malformed and will be skipped.')\n",
    "            except Exception as error:\n",
    "                logging.error(f'Error reading {file_name} => {error}')\n",
    "    \n",
    "    if df_list:\n",
    "        logging.info('Concatenating dataframes')\n",
    "        return pd.concat(df_list, ignore_index=True)\n",
    "    else:\n",
    "        logging.warning('No dataframes to concatenate, returning an empty DataFrame.')\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_BCP_PATH2 = Path(OUT_BCP_PATH)\n",
    "\n",
    "def parse_and_dump(\n",
    "    df: pd.DataFrame, \n",
    "    bcp_file_name: str, \n",
    "    table_name: str, \n",
    "    columns: Optional[List[str]] = None, \n",
    "    reload: bool = True, \n",
    "    drop_table: bool = False, \n",
    "    sep: str = '\\t',\n",
    "    extract_year: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generic method to parse DataFrame and dump to a file with error handling.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame to be processed.\n",
    "        bcp_file_name (str): Name of the BCP file.\n",
    "        table_name (str): Name of the table to dump data.\n",
    "        columns (Optional[List[str]]): List of columns to reindex the DataFrame.\n",
    "        reload (bool): Flag to reload data.\n",
    "        drop_table (bool): Flag to drop the table before dumping.\n",
    "        sep (str): Separator for the CSV file.\n",
    "        extract_year (bool): Flag to extract the year from 'dc_date_issued' column.\n",
    "    \"\"\"\n",
    "    bcp_file_path = OUT_BCP_PATH2 / bcp_file_name\n",
    "    skipped_file_path = OUT_BCP_PATH2 / f'skipped_{bcp_file_name}'\n",
    "\n",
    "    if reload:\n",
    "        if columns:\n",
    "            df = df[df.columns.intersection(set(columns))]\n",
    "            df = df.reindex(columns=columns)\n",
    "        if extract_year:\n",
    "            df['rc_year'] = df['dc_date_issued'].str.extract(r'([0-9]{4})', expand=True)\n",
    "        df.to_csv(bcp_file_path, sep=sep, index=False, index_label='\\t', header=False)\n",
    "        logging.info(f'== INFO - DONE - Wrote data to file {bcp_file_path}')\n",
    "\n",
    "    try_again = -1\n",
    "    count = 0\n",
    "\n",
    "    while try_again != 0:\n",
    "        logging.info(f\"== START Try Again {count}\")\n",
    "        try_again = dump_table(table_name=table_name, bcp_file=bcp_file_path, columns=columns or df.columns.to_list(), drop_table=drop_table, sep=sep)\n",
    "        logging.info(f'{try_again=}')\n",
    "        if try_again > 0:\n",
    "            count += 1\n",
    "            with open(bcp_file_path, \"r+\") as f:\n",
    "                lines = f.readlines()\n",
    "                f.seek(0)\n",
    "                skipped_row = lines.pop(try_again - 1)\n",
    "                logging.info(f\"INFO - Removed row: {skipped_row.strip()}\")\n",
    "                f.truncate()\n",
    "                f.writelines(lines)\n",
    "            with open(skipped_file_path, \"a\") as f2:\n",
    "                f2.write(skipped_row)\n",
    "        elif try_again == -1:\n",
    "            logging.error(\"ERROR - Unrecoverable error during dumping table\")\n",
    "            break\n",
    "    else:\n",
    "        logging.info(\"== Try Again DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 266038 entries, 0 to 266037\n",
      "Columns: 134 entries, dc_contributor_author to ethz_date_openbisupload\n",
      "dtypes: object(134)\n",
      "memory usage: 272.0+ MB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_237868/1609896663.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  big_df['handle_id'] = big_df['dc_identifier_uri'].apply(lambda uri: uri.split(\"/\")[-1] if pd.notna(uri) else uri)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    266036\n",
      "True          2\n",
      "Name: count, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 266038 entries, 0 to 266037\n",
      "Columns: 135 entries, dc_contributor to rc_item_id\n",
      "dtypes: object(135)\n",
      "memory usage: 274.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# thse steps can be done year by year or as a one big dataset\n",
    "big_df = bulk_load_df()\n",
    "reordered_df = big_df.reindex(columns=ALL_COLUMNS_SORTED)\n",
    "print(big_df.info())\n",
    "big_df['ethz_size'] = big_df['ethz_size'].apply(lambda size: size.replace('\\t', \" \") if pd.notna(size) else size)\n",
    "big_df['handle_id'] = big_df['dc_identifier_uri'].apply(lambda uri: uri.split(\"/\")[-1] if pd.notna(uri) else uri)\n",
    "big_df['dc_title'] = big_df['dc_title'].apply(lambda title: title.replace('\\\\', \"\") if pd.notna(title) else title)\n",
    "\n",
    "reordered_df = big_df.reindex(columns=ALL_COLUMNS_SORTED)\n",
    "print(reordered_df.duplicated(subset=['handle_id']).value_counts())\n",
    "#print(reordered_df.duplicated(subset=['handle_id']))\n",
    "reordered_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== INFO - Executed table creation for RCLeitzahl\n",
      "== INFO - cursor and connection CLOSED.\n"
     ]
    }
   ],
   "source": [
    "leitzhal_df = reordered_df[['handle_id', 'rc_item_id','ethz_leitzahl', 'ethz_leitzahlidentifiers', 'ethz_leitzahl_certified', 'ethz_leitzahlidentifiers_certified']]\n",
    "parse_leitzahl(leitzhal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== INFO - Executed table creation for RCAuthorship\n",
      "== INFO - cursor and connection CLOSED.\n"
     ]
    }
   ],
   "source": [
    "AUTHORSHIP_BCP_FILE = 'authorship.bcp'\n",
    "bcp_file_path = OUT_BCP_PATH+AUTHORSHIP_BCP_FILE\n",
    "contributors_set, local_authorship_list = parse_contributors(reordered_df)\n",
    "global_authorship_df = pd.DataFrame(local_authorship_list, columns=AUTHORSHIP_COLUMNS)\n",
    "global_authorship_df.to_csv(bcp_file_path, sep='\\t', index=False, index_label='\\t', header=False)\n",
    "\n",
    "parse_and_dump(global_authorship_df, AUTHORSHIP_BCP_FILE, AUTHORSHIP_TABLE, AUTHORSHIP_COLUMNS, drop_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== INFO - Executed table creation for RCPublication\n",
      "== INFO - cursor and connection CLOSED.\n"
     ]
    }
   ],
   "source": [
    "parse_and_dump(reordered_df, 'publications.bcp', PUBLICATION_TABLE, PUBLICATION_COLUMNS, extract_year=True, drop_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== INFO - Executed table creation for RCMasterTable\n",
      "== INFO - cursor and connection CLOSED.\n"
     ]
    }
   ],
   "source": [
    "parse_and_dump(reordered_df, 'master.bcp', MASTER_TABLE, drop_table=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biblio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
