# Technical Documentation: Bibliographic Data Processing Pipeline

This document outlines the technical details of the bibliographic data processing pipeline. The system is designed to ingest, process, and store bibliographic data from various sources like Scopus and Web of Science (WoS).

## 1. Overall Architecture

The pipeline is orchestrated by a main shell script (`controller/process.sh`) that manages various stages of data handling: environment setup, data ingestion, data loading into a database, and post-processing. It leverages a combination of shell scripts for control flow and database operations, and Python scripts for data parsing and transformation. Parallel processing is employed to efficiently handle large volumes of data. A PostgreSQL database is used for storing file processing statuses, staging data, and final production data.

## 2. Core Components and Workflow

### 2.1. Controller Scripts (`controller/`)

The `controller/` directory contains shell scripts that manage the execution of the pipeline:

*   **`process.sh`**:
    *   The main entry point and orchestrator of the entire pipeline.
    *   Takes arguments: `provider` (e.g., Scopus, WoS), `source_dir` (input data location), `work_dir` (temporary working directory), and `update_refresh` (specifies 'update' or 'refresh' mode).
    *   Initializes environment variables (including database settings via `db/set_biblio_db_envs.sh`).
    *   Cleans the working directory (`controller/clean_env.sh`).
    *   Prepares for data update or refresh:
        *   **Update Mode**: Calls `controller/prepare_update.sh` to handle incremental updates.
        *   **Refresh Mode**: Recreates staging tables using provider-specific scripts (e.g., `db/scopus/RecreateAllStagingTables.sh`).
    *   Launches the `controller/load_daemon.sh` in the background to manage data loading into the database.
    *   Initiates parallel data ingestion by running multiple instances of `libbiblio/ingest_data.py`.
    *   Monitors the `BibliometricsFileStatus` database table to manage the pool of ingestion processes.
    *   Once ingestion is complete, signals the `load_daemon.sh`.
    *   Waits for the `load_daemon.sh` to finish loading data into staging tables.
    *   Triggers post-processing scripts:
        *   `db/<provider>/ProcessAllStaging.sh`: Moves data from staging to production tables, performs transformations.
        *   `db/<provider>/CreateAllIndexes.sh`: Creates database indexes on production tables.

*   **`load_daemon.sh`**:
    *   Runs as a background process.
    *   Responsible for loading data from BCP (Bulk Copy Program) files (generated by Python ingestors) into the database staging tables.
    *   Receives a "MARK_COMPLETE" signal from `process.sh` once all files have been passed to Python for ingestion, to know when to finalize its operations.

*   **`prepare_update.sh`**:
    *   Handles the logic for preparing an "update" run. This likely involves identifying new or changed files in the `source_dir` and setting them up in the `work_dir/updates` directory for processing.

*   **`clean_env.sh`**:
    *   A utility script to clean the specified working directory before a new processing run.

### 2.2. Python Ingestion Engine (`libbiblio/`)

*   **`libbiblio/ingest_data.py`**:
    *   The core Python script for parsing and preparing data for ingestion.
    *   Launched in multiple parallel instances by `process.sh`.
    *   **Arguments**: `provider` (e.g., Scopus, WoS) and `bcp_dir` (directory for outputting BCP files).
    *   **Provider-Specific Ingestors**: Dynamically selects ingestor functions (e.g., `scopus_ingest_zip`, `wos_ingest_xml`) based on the `provider`. These functions are located in `libbiblio/sources/<provider>/<provider>_ingestor.py`.
    *   **File Handling Coordination**:
        *   Uses helper functions from `libbiblio/file_status_handler.py` (`get_next_file`, `set_as_processed`).
        *   Continuously queries the `BibliometricsFileStatus` database table to get the next available input file (ZIP or XML) for the specified `provider`.
        *   The `process_dir_zip_or_xml` function within `ingest_data.py` determines the file type and calls the appropriate ingestor function.
    *   **Data Transformation**: The provider-specific ingestor functions parse the raw data files (XML within ZIPs, or standalone XMLs) and transform the data into a structured format, writing it out to BCP files in the `bcp_dir`.
    *   **Status Update**: After successfully processing a file and generating BCP files, it updates the file's status to 'processed' in the `BibliometricsFileStatus` table.
    *   **Process Lifetime**: Each Python process runs for a maximum configurable time (e.g., 0.5 hours) to prevent slowdowns from long-running instances, then exits. `process.sh` will restart processes as needed if there's more work.

*   **`libbiblio/file_status_handler.py`**:
    *   Contains functions to interact with the `BibliometricsFileStatus` table.
    *   `get_next_file(provider)`: Fetches an unprocessed file for the given provider, marks it as 'processing'.
    *   `set_as_processed(provider, path_to_process)`: Updates the status of a file to 'processed'.

*   **`libbiblio/sources/<provider>/`**:
    *   These directories contain the specific logic for handling data from different providers.
    *   `<provider>_ingestor.py`: Contains functions like `<provider>_ingest_zip` and `<provider>_ingest_xml` that know how to open and trigger parsing of the specific file formats for that provider.
    *   `<provider>_parser.py`: (Assumed, based on common patterns, e.g., `scopus_parser.py`) Contains the detailed parsing logic to extract relevant information from the source files (e.g., authors, publications, citations).

### 2.3. Database Interaction (`db/`)

The `db/` directory and its subdirectories (`db/scopus/`, `db/wos/`) are crucial for data storage, schema management, and processing.

*   **`db/set_biblio_db_envs.sh`**:
    *   Sets essential environment variables for database connection (server, database name, user, password). This script is sourced by other scripts that need database access.

*   **BCP Files**:
    *   Intermediate files generated by the Python ingestors (`ingest_data.py`).
    *   Contain data in a format suitable for fast bulk loading into the database.
    *   Stored temporarily in the `work_dir/bcp` directory.

*   **`BibliometricsFileStatus` Table**:
    *   A central table used to manage and track the processing status of each input file.
    *   Columns likely include `file_name`, `file_path`, `provider`, and `status` (e.g., 'unprocessed', 'processing', 'processed', 'error').

*   **Staging Tables**:
    *   Temporary tables in the database where data from BCP files is initially loaded by `load_daemon.sh`.
    *   Schema definitions for these are likely found in SQL files within `db/<provider>/` (e.g., `WoSAuthorshipStaging.sql`).
    *   `db/<provider>/RecreateAllStagingTables.sh`: Scripts to drop and recreate these staging tables during a 'refresh' run.

*   **Production Tables**:
    *   The final tables holding the cleaned, processed bibliographic data.
    *   Schema definitions are in SQL files (e.g., `ScopusPublication.sql`, `WoSAuthor.sql`).

*   **`db/<provider>/ProcessAllStaging.sh`**:
    *   A crucial script that executes SQL commands to:
        *   Transfer data from staging tables to production tables.
        *   Perform data cleaning, transformations, and deduplication.
        *   Link related entities (e.g., authors to publications, publications to citations).

*   **`db/<provider>/CreateAllIndexes.sh`**:
    *   Executes SQL commands to create indexes on the production tables. This is vital for optimizing query performance once the data is loaded.

*   **SQL Files (`.sql`)**:
    *   Contain DDL (Data Definition Language) for creating tables, views, and indexes.
    *   Contain DML (Data Manipulation Language) used by scripts like `ProcessAllStaging.sh` for data transformation.

## 3. Data Flow Summary

1.  **Initiation**: `controller/process.sh` is manually executed.
2.  **Environment Setup**: `process.sh` sets up the environment and working directories.
3.  **Daemon Start**: `load_daemon.sh` starts to listen for BCP files.
4.  **File Listing (Update/Refresh specific)**:
    *   **Refresh**: Staging tables are recreated. All files in `source_dir` are listed in `BibliometricsFileStatus` as 'unprocessed'.
    *   **Update**: `prepare_update.sh` identifies new/changed files, which are then listed in `BibliometricsFileStatus`.
5.  **Parallel Ingestion**: `process.sh` launches multiple `libbiblio/ingest_data.py` instances.
    *   Each `ingest_data.py` instance gets an 'unprocessed' file from `BibliometricsFileStatus`.
    *   The file is parsed by provider-specific ingestors/parsers.
    *   Data is written to BCP files in `work_dir/bcp/`.
    *   File status is updated to 'processed'.
6.  **BCP Loading (Concurrent)**: As BCP files appear, `load_daemon.sh` loads them into corresponding staging tables in the database.
7.  **Staging to Production**: Once all files are ingested and loaded into staging tables (signaled via `load_daemon.sh MARK_COMPLETE` and `wait`), `process.sh` executes `db/<provider>/ProcessAllStaging.sh`. This script cleans and moves data from staging tables to the final production tables.
8.  **Indexing**: `process.sh` executes `db/<provider>/CreateAllIndexes.sh` to build indexes on the production data.
9.  **Completion**: The pipeline finishes.

## 4. Key Directory Structures

*   **`controller/`**: Contains master control scripts for the pipeline.
*   **`db/`**:
    *   Contains database schema files (.sql), environment setup (`set_biblio_db_envs.sh`), and scripts for data manipulation within the database (staging, indexing).
    *   **`db/<provider>/`**: Provider-specific SQL scripts for table creation, staging processing, and index creation.
*   **`libbiblio/`**:
    *   Contains the Python-based data ingestion engine.
    *   **`libbiblio/sources/<provider>/`**: Provider-specific Python modules for parsing and ingesting data (e.g., `scopus_ingestor.py`, `wos_parser.py`).
*   **`work_dir/` (User-defined, e.g., `/tmp/biblio_work/`)**:
    *   **`work_dir/bcp/`**: Temporary storage for BCP files generated by Python.
    *   **`work_dir/updates/`**: (In update mode) Contains files prepared for an update run.
*   **`source_dir/` (User-defined, e.g., `/data/scopus_raw/`)**: Location of the raw input data files (ZIPs, XMLs).

This documentation provides a snapshot based on the current understanding of the codebase. Further detailed analysis of individual SQL scripts and Python parsing logic would be needed for a more granular understanding.
